from typing import Any, Optional, Literal
import json
import asyncio
import os
import sys
from datetime import datetime
import logging
from mcp.server.fastmcp import FastMCP

# ä¸¥æ ¼æ¨¡å¼ï¼šå°†æ‰€æœ‰ stderr é‡å®šå‘åˆ°æœ¬åœ°æ—¥å¿—æ–‡ä»¶ï¼Œé¿å…ä¸ MCP stdio å†²çª
_original_stdout = sys.stdout  # ä¿å­˜åŸå§‹ stdout ä¾› MCP ä½¿ç”¨
try:
    os.makedirs(os.path.join('local_data', 'logs'), exist_ok=True)
    _stderr_path = os.path.join('local_data', 'logs', 'mcp_server.stderr.log')
    _stderr_fp = open(_stderr_path, 'a', encoding='utf-8', buffering=1)
    sys.stderr = _stderr_fp  # type: ignore[assignment]
    
    # åŒæ—¶é‡å®šå‘ stdout åˆ° stderrï¼Œé˜²æ­¢ä»»ä½•æ„å¤–çš„ stdout è¾“å‡ºæ±¡æŸ“ MCP åè®®
    sys.stdout = _stderr_fp  # type: ignore[assignment]
except Exception:
    # å³ä½¿æ—¥å¿—æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼Œä¹Ÿä¸è¦ä¸­æ–­æœåŠ¡å™¨å¯åŠ¨
    pass

# åœ¨ stderr é‡å®šå‘ä¹‹åå¯¼å…¥æ¨¡å—ï¼Œé¿å…å¯¼å…¥æ—¶çš„è¾“å‡ºæ±¡æŸ“ stdout
from tapd_data_fetcher import get_story_msg, get_bug_msg    # ä»tapd_data_fetcheræ¨¡å—å¯¼å…¥è·å–éœ€æ±‚å’Œç¼ºé™·æ•°æ®çš„å‡½æ•°
from mcp_tools.example_tool import example_function    # ä»mcp_tools.example_toolæ¨¡å—å¯¼å…¥ç¤ºä¾‹å·¥å…·å‡½æ•°
from mcp_tools.data_vectorizer import vectorize_tapd_data, search_tapd_data, get_vector_db_info    # å¯¼å…¥ä¼˜åŒ–åçš„å‘é‡åŒ–å·¥å…·
from mcp_tools.fake_tapd_gen import generate as fake_generate    # å¯¼å…¥TAPDæ•°æ®ç”Ÿæˆå™¨
from mcp_tools.context_optimizer import build_overview    # å¯¼å…¥ä¸Šä¸‹æ–‡ä¼˜åŒ–å™¨
from mcp_tools.docx_summarizer import summarize_docx as _summarize_docx
from mcp_tools.word_frequency_analyzer import analyze_tapd_word_frequency    # å¯¼å…¥è¯é¢‘åˆ†æå™¨
from mcp_tools.data_preprocessor import preprocess_description_field, preview_description_cleaning    # å¯¼å…¥æ•°æ®é¢„å¤„ç†å·¥å…·
from mcp_tools.knowledge_base import enhance_tapd_data_with_knowledge    # å¯¼å…¥æ•°æ®å¢å¼ºå·¥å…·
from mcp_tools.time_trend_analyzer import analyze_time_trends as analyze_trends    # å¯¼å…¥æ—¶é—´è¶‹åŠ¿åˆ†æå·¥å…·
from mcp_tools.data_precise_searcher import search_tapd_data as precise_search, search_by_priority, get_tapd_statistics    # å¯¼å…¥ç²¾ç¡®æœç´¢å·¥å…·

# å…¨å±€æ—¥å¿—ä¸ç¯å¢ƒé™å™ªï¼šç¡®ä¿ç¬¬ä¸‰æ–¹åº“ä¸ä¼šå‘ stdout æ‰“å°ï¼Œé¿å…ç ´å MCP stdio
logging.basicConfig(level=logging.WARNING, stream=sys.stderr, force=True)
for _name, _level in (
    ("sentence_transformers", logging.ERROR),
    ("transformers", logging.ERROR),
    ("torch", logging.ERROR),
    ("faiss", logging.ERROR),
    ("urllib3", logging.ERROR),
):
    try:
        _lg = logging.getLogger(_name)
        _lg.setLevel(_level)
        if not _lg.handlers:
            _h = logging.StreamHandler(sys.stderr)
            _h.setLevel(_level)
            _lg.addHandler(_h)
        _lg.propagate = False
    except Exception:
        pass

# åˆå§‹åŒ–MCPæœåŠ¡å™¨
mcp = FastMCP("tapd")

@mcp.tool()
async def example_tool(param1: str = "success", param2: int = 57257) -> dict:
    """
    ç¤ºä¾‹å·¥å…·å‡½æ•°ï¼ˆç”¨äºæ¼”ç¤ºMCPå·¥å…·æ³¨å†Œæ–¹å¼ï¼‰
    
    åŠŸèƒ½æè¿°:
        - å±•ç¤ºå¦‚ä½•åˆ›å»ºå’Œæ³¨å†Œæ–°çš„MCPå·¥å…·
        - æ¼”ç¤ºå‚æ•°ä¼ é€’å’Œè¿”å›å€¼å¤„ç†
        
    å‚æ•°:
        param1 (str): ç¤ºä¾‹å­—ç¬¦ä¸²å‚æ•°ï¼Œå°†è¢«è½¬æ¢ä¸ºå¤§å†™
        param2 (int): ç¤ºä¾‹æ•´å‹å‚æ•°ï¼Œå°†è¢«ä¹˜ä»¥2
        
    è¿”å›:
        dict: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¸º:
            {
                "processed_str": å¤„ç†åçš„å­—ç¬¦ä¸²,
                "processed_num": å¤„ç†åçš„æ•°å­—,
                "combined": ç»„åˆåçš„ç»“æœ
            }
    """
    return await example_function(param1, param2)

@mcp.tool()
async def get_tapd_data() -> str:
    """ä»TAPD APIè·å–éœ€æ±‚å’Œç¼ºé™·æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
    
    åŠŸèƒ½æè¿°:
        - ä»TAPD APIè·å–å®Œæ•´çš„éœ€æ±‚å’Œç¼ºé™·æ•°æ®
        - å°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ local_data/msg_from_fetcher.json
        - è¿”å›è·å–åˆ°çš„éœ€æ±‚å’Œç¼ºé™·æ•°é‡ç»Ÿè®¡
        - ä¸ºåç»­çš„æœ¬åœ°æ•°æ®åˆ†ææä¾›æ•°æ®åŸºç¡€
        
    æ•°æ®ä¿å­˜æ ¼å¼:
        {
            "stories": [...],  // éœ€æ±‚æ•°æ®æ•°ç»„
            "bugs": [...]      // ç¼ºé™·æ•°æ®æ•°ç»„
        }
        
    è¿”å›:
        str: åŒ…å«æ•°æ®è·å–ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
        
    ä½¿ç”¨åœºæ™¯:
        - åˆæ¬¡è®¾ç½®æ—¶è·å–æœ€æ–°æ•°æ®
        - å®šæœŸæ›´æ–°æœ¬åœ°æ•°æ®ç¼“å­˜
        - ä¸ºç¦»çº¿åˆ†æå‡†å¤‡æ•°æ®
    """
    try:
        print('===== Start fetching stories =====', file=sys.stderr, flush=True)
        stories_data = await get_story_msg()

        print('===== Start fetching bugs =====', file=sys.stderr, flush=True)
        bugs_data = await get_bug_msg()

        # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
        data_to_save = {
            'stories': stories_data,
            'bugs': bugs_data
        }

        # ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶ä¿å­˜æ•°æ®
        os.makedirs('local_data', exist_ok=True)
        with open(os.path.join('local_data', 'msg_from_fetcher.json'), 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=4)

        # è¿”å›ç»Ÿè®¡ç»“æœ
        result = {
            "status": "success",
            "message": "æ•°æ®å·²æˆåŠŸä¿å­˜è‡³local_data/msg_from_fetcher.jsonæ–‡ä»¶",
            "statistics": {
                "stories_count": len(stories_data),
                "bugs_count": len(bugs_data),
                "total_count": len(stories_data) + len(bugs_data)
            },
            "file_path": "local_data/msg_from_fetcher.json"
        }

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"è·å–å’Œä¿å­˜TAPDæ•°æ®å¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®å’Œç½‘ç»œè¿æ¥"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def get_tapd_stories() -> str:
    """è·å–TAPDå¹³å°æŒ‡å®šé¡¹ç›®çš„éœ€æ±‚æ•°æ®ï¼ˆæ”¯æŒåˆ†é¡µï¼‰
    
    åŠŸèƒ½æè¿°:
        - ä»TAPD APIè·å–æŒ‡å®šé¡¹ç›®çš„æ‰€æœ‰éœ€æ±‚æ•°æ®
        - æ”¯æŒåˆ†é¡µè·å–å¤§é‡æ•°æ®
        - è‡ªåŠ¨å¤„ç†APIè®¤è¯å’Œé”™è¯¯
        - æ•°æ®ä¸ä¿å­˜è‡³æœ¬åœ°ï¼Œå»ºè®®ä»…åœ¨æ•°æ®é‡è¾ƒå°æ—¶ä½¿ç”¨
        
    è¿”å›æ•°æ®æ ¼å¼:
        - æ¯ä¸ªéœ€æ±‚åŒ…å«IDã€æ ‡é¢˜ã€çŠ¶æ€ã€ä¼˜å…ˆçº§ã€åˆ›å»º/ä¿®æ”¹æ—¶é—´ç­‰å­—æ®µ
        - æ•°æ®å·²æŒ‰JSONæ ¼å¼åºåˆ—åŒ–ï¼Œç¡®ä¿AIå®¢æˆ·ç«¯å¯ç›´æ¥è§£æ
        
    Returns:
        str: æ ¼å¼åŒ–åçš„éœ€æ±‚æ•°æ®JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ä¸­æ–‡å†…å®¹
    """
    try:
        stories = await get_story_msg()
        return json.dumps(stories, ensure_ascii=False, indent=2)
    except Exception as e:
        return f"è·å–éœ€æ±‚æ•°æ®å¤±è´¥ï¼š{str(e)}"

@mcp.tool()
async def get_tapd_bugs() -> str:
    """è·å–TAPDå¹³å°æŒ‡å®šé¡¹ç›®çš„ç¼ºé™·æ•°æ®ï¼ˆæ”¯æŒåˆ†é¡µï¼‰
    
    åŠŸèƒ½æè¿°:
        - ä»TAPD APIè·å–æŒ‡å®šé¡¹ç›®çš„æ‰€æœ‰ç¼ºé™·æ•°æ®
        - æ”¯æŒæŒ‰çŠ¶æ€ã€ä¼˜å…ˆçº§ç­‰æ¡ä»¶è¿‡æ»¤
        - è‡ªåŠ¨å¤„ç†APIè®¤è¯å’Œé”™è¯¯
        - æ•°æ®ä¸ä¿å­˜è‡³æœ¬åœ°ï¼Œå»ºè®®ä»…åœ¨æ•°æ®é‡è¾ƒå°æ—¶ä½¿ç”¨
        
    è¿”å›æ•°æ®æ ¼å¼:
        - æ¯ä¸ªç¼ºé™·åŒ…å«IDã€æ ‡é¢˜ã€ä¸¥é‡ç¨‹åº¦ã€çŠ¶æ€ã€è§£å†³æ–¹æ¡ˆç­‰å­—æ®µ
        - æ•°æ®å·²æŒ‰JSONæ ¼å¼åºåˆ—åŒ–ï¼Œç¡®ä¿AIå®¢æˆ·ç«¯å¯ç›´æ¥è§£æ
        
    Returns:
        str: æ ¼å¼åŒ–åçš„ç¼ºé™·æ•°æ®JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ä¸­æ–‡å†…å®¹
    """
    try:
        bugs = await get_bug_msg()
        return json.dumps(bugs, ensure_ascii=False, indent=2)
    except Exception as e:
        return f"è·å–ç¼ºé™·æ•°æ®å¤±è´¥ï¼š{str(e)}"

@mcp.tool()
async def vectorize_data(
    data_file_path: Optional[str] = "local_data/msg_from_fetcher.json",
    chunk_size: int = 10,
    timeout_seconds: int = 600
) -> str:
    """å‘é‡åŒ–TAPDæ•°æ®ä»¥æ”¯æŒå¤§æ‰¹é‡æ•°æ®å¤„ç†
    
    åŠŸèƒ½æè¿°:
        - å°†è·å–åˆ°çš„çš„TAPDæ•°æ®è¿›è¡Œå‘é‡åŒ–
        - è§£å†³å¤§æ‰¹é‡æ•°æ®å¤„ç†æ—¶tokensè¶…é™é—®é¢˜
        - æ•°æ®åˆ†ç‰‡å¤„ç†ï¼Œæé«˜æ£€ç´¢æ•ˆç‡
        - ä½¿ç”¨SentenceTransformerså’ŒFAISSæ„å»ºå‘é‡æ•°æ®åº“
        
    å‚æ•°:
        data_file_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º local_data/msg_from_fetcher.json
        chunk_size (int): åˆ†ç‰‡å¤§å°ï¼Œæ¯ä¸ªåˆ†ç‰‡åŒ…å«çš„æ¡ç›®æ•°ï¼Œé»˜è®¤10æ¡
            - æ¨èå€¼ï¼š10-20ï¼ˆå¹³è¡¡ç²¾åº¦ä¸æ•ˆç‡ï¼‰
            - è¾ƒå°å€¼ï¼šæœç´¢æ›´ç²¾å‡†ï¼Œä½†åˆ†ç‰‡æ›´å¤š
            - è¾ƒå¤§å€¼ï¼šå‡å°‘åˆ†ç‰‡æ•°é‡ï¼Œä½†å¯èƒ½é™ä½æœç´¢ç²¾åº¦
        
    è¿”å›:
        str: å‘é‡åŒ–å¤„ç†ç»“æœçš„JSONå­—ç¬¦ä¸²
        
    ä½¿ç”¨åœºæ™¯:
        - åˆæ¬¡è®¾ç½®æˆ–æ•°æ®æ›´æ–°æ—¶è°ƒç”¨
        - ä¸ºåç»­çš„æ™ºèƒ½æœç´¢å’Œç›¸ä¼¼åº¦åŒ¹é…åšå‡†å¤‡
    """
    try:
        # normalize inputs
        effective_path = data_file_path if (data_file_path and str(data_file_path).strip()) else "local_data/msg_from_fetcher.json"
        safe_chunk = chunk_size if isinstance(chunk_size, int) and chunk_size > 0 else 10
        safe_timeout = max(0, int(timeout_seconds)) if isinstance(timeout_seconds, int) else 600

        print(
            f"[MCP {datetime.now().strftime('%H:%M:%S')}] vectorize_data start, file={effective_path}, chunk_size={safe_chunk}, timeout={safe_timeout or 'no'}s",
            file=sys.stderr,
            flush=True,
        )

        # Force same-thread execution for MCP Inspector compatibility; subprocess has stderr redirection issues
        use_subprocess = os.getenv("VEC_USE_SUBPROCESS", "0").strip() in ("1", "true", "True")

        if not use_subprocess:
            # Use more aggressive timeout for MCP Inspector compatibility
            effective_timeout = min(safe_timeout, 60) if safe_timeout > 0 else 60
            try:
                async def _do_vectorize():
                    return await vectorize_tapd_data(effective_path, safe_chunk)

                result = await asyncio.wait_for(_do_vectorize(), timeout=effective_timeout)
            except asyncio.TimeoutError:
                print(
                    f"[MCP {datetime.now().strftime('%H:%M:%S')}] vectorize_data timeout (> {effective_timeout}s)",
                    file=sys.stderr,
                    flush=True,
                )
                result = {
                    "status": "error",
                    "message": f"Vectorization timeout after {effective_timeout}s, try again later or reduce chunk_size",
                    "data_file_path": effective_path,
                    "timeout_seconds": effective_timeout,
                }
            except Exception as e:
                result = {"status": "error", "message": f"Vectorization failed: {e}"}
        else:
            py_exe = sys.executable or "python"
            cmd = [py_exe, "-m", "mcp_tools.vec_worker", "--file", effective_path, "--chunk", str(safe_chunk)]
            try:
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                try:
                    if safe_timeout > 0:
                        stdout_bytes, stderr_bytes = await asyncio.wait_for(proc.communicate(), timeout=safe_timeout)
                    else:
                        stdout_bytes, stderr_bytes = await proc.communicate()
                except asyncio.TimeoutError:
                    try:
                        proc.kill()
                    except Exception:
                        pass
                    print(
                        f"[MCP {datetime.now().strftime('%H:%M:%S')}] vectorize_data timeout (>{safe_timeout}s)",
                        file=sys.stderr,
                        flush=True,
                    )
                    result = {
                        "status": "error",
                        "message": "Vectorization timeout, try again later or reduce chunk_size",
                        "data_file_path": effective_path,
                        "timeout_seconds": safe_timeout,
                    }
                else:
                    if stderr_bytes:
                        try:
                            sys.stderr.write(stderr_bytes.decode(errors="replace"))
                            sys.stderr.flush()
                        except Exception:
                            pass
                    code = proc.returncode
                    text = (stdout_bytes or b"").decode(errors="replace").strip()
                    if code != 0:
                        result = {"status": "error", "message": f"Worker exit code {code}", "raw": text[:4000]}
                    elif not text:
                        result = {"status": "error", "message": "Worker produced no output"}
                    else:
                        try:
                            result = json.loads(text)
                        except Exception as e:
                            result = {"status": "error", "message": f"Invalid worker output: {e}", "raw": text[:4000]}
            except FileNotFoundError as e:
                result = {"status": "error", "message": f"Python executable not found: {e}"}
            except Exception as e:
                result = {"status": "error", "message": f"Failed to run worker: {e}"}

        # normalize response shape and message
        if isinstance(result, str):
            try:
                result = json.loads(result)
            except Exception:
                result = {"status": "error", "message": "Invalid result payload"}

        if isinstance(result, dict):
            res: dict[str, Any] = dict(result)
            res.setdefault("data_file_path", effective_path)
            if res.get("status") == "success":
                # ensure success message and attach vector DB readiness info (best-effort)
                res["message"] = res.get("message") or "Vectorization completed"
                try:
                    info = await get_vector_db_info()
                    status_val = info.get("status")
                    if status_val is not None:
                        res["vector_db_status"] = str(status_val)
                    db_stats = info.get("stats")
                    if db_stats is not None and res.get("stats") is None:
                        if isinstance(db_stats, dict):
                            res["stats"] = db_stats  # type: ignore[assignment]
                except Exception:
                    pass
            result = res

        print(
            f"[MCP {datetime.now().strftime('%H:%M:%S')}] vectorize_data end, status={result.get('status') if isinstance(result, dict) else 'unknown'}",
            file=sys.stderr,
            flush=True,
        )
        
        # Ensure MCP Inspector compatibility by returning a structured response
        if isinstance(result, dict) and result.get("status") == "success":
            # Extract stats for display (check both direct keys and nested stats)
            stats = result.get("stats", {})
            chunks_count = result.get("chunks", stats.get("total_chunks", "?"))
            vector_dim = result.get("dim", stats.get("vector_dimension", "?"))
            total_items = result.get("total_items", stats.get("total_items", "?"))
            elapsed_time = result.get("elapsed_time", result.get("elapsed_seconds", "?"))
            
            # Add user-friendly status message for MCP Inspector
            display_message = "âœ… å‘é‡åŒ–æˆåŠŸå®Œæˆ!\n\n"
            if chunks_count != "?":
                display_message += f"ğŸ“¦ å¤„ç†åˆ†ç‰‡æ•°: {chunks_count}\n"
            if vector_dim != "?":
                display_message += f"ğŸ”¢ å‘é‡ç»´åº¦: {vector_dim}\n"
            if total_items != "?":
                display_message += f"ğŸ“„ æ€»é¡¹ç›®æ•°: {total_items}\n"
            if elapsed_time != "?":
                display_message += f"â±ï¸ è€—æ—¶: {elapsed_time}ç§’\n"
            
            # Create structured response that MCP Inspector can display
            mcp_result = {
                "status": "success",
                "message": display_message.strip(),
                "details": result,
                "summary": f"å‘é‡åŒ–å®Œæˆ - {chunks_count}ä¸ªåˆ†ç‰‡, {vector_dim}ç»´å‘é‡"
            }
            return json.dumps(mcp_result, ensure_ascii=False, indent=2)
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[MCP {datetime.now().strftime('%H:%M:%S')}] vectorize_data exception: {e}", file=sys.stderr, flush=True)
        error_result = {
            "status": "error",
            "message": f"Vectorization failed: {str(e)}"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool() 
async def get_vector_info() -> str:
    """è·å–å‘é‡æ•°æ®åº“çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
    
    åŠŸèƒ½æè¿°:
        - æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å·²å»ºç«‹
        - è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¡ç›®æ•°ã€åˆ†ç‰‡æ•°ç­‰ï¼‰
        - å¸®åŠ©äº†è§£å½“å‰æ•°æ®å¤„ç†èƒ½åŠ›
        
    è¿”å›:
        str: æ•°æ®åº“ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
        
    ç»Ÿè®¡ä¿¡æ¯åŒ…æ‹¬:
        - æ€»åˆ†ç‰‡æ•°å’Œæ¡ç›®æ•°
        - éœ€æ±‚å’Œç¼ºé™·çš„åˆ†ç‰‡åˆ†å¸ƒ
        - å‘é‡ç»´åº¦å’Œå­˜å‚¨è·¯å¾„
    """
    try:
        result = await get_vector_db_info()
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"è·å–ä¿¡æ¯å¤±è´¥ï¼š{str(e)}"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
async def search_data(query: str, top_k: int = 5) -> str:
    """åœ¨å‘é‡åŒ–çš„TAPDæ•°æ®ä¸­è¿›è¡Œæ™ºèƒ½æœç´¢
    
    åŠŸèƒ½æè¿°:
        - åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ç›¸å…³çš„éœ€æ±‚å’Œç¼ºé™·
        - æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œæ— éœ€ç²¾ç¡®åŒ¹é…å…³é”®è¯
        - è¿”å›â€œå‰ä¸¤æ‰¹â€æœ€ç›¸å…³çš„åˆ†ç‰‡ï¼Œæ¯æ‰¹åŒ…å« top_k æ¡åŸå§‹æ¡ç›®
        - æœ‰æ•ˆè§£å†³å¤§æ‰¹é‡æ•°æ®åˆ†æçš„tokené™åˆ¶é—®é¢˜
        
    å‚æ•°:
        query (str): æœç´¢æŸ¥è¯¢ï¼Œæ”¯æŒä¸­æ–‡è‡ªç„¶è¯­è¨€æè¿°
        top_k (int): æ¯æ‰¹è¿”å›çš„åŸå§‹æ¡ç›®æ•°é‡ã€‚æœ€ç»ˆè¿”å›ä¸¤æ‰¹æ•°æ®ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦çš„ä¸¤ä¸ªåˆ†ç‰‡ï¼‰ï¼Œæ¯æ‰¹ top_k æ¡ã€‚
        
    è¿”å›:
        str: æœç´¢ç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ï¼š
            - batches: è¿”å›çš„æ‰¹æ¬¡æ•°ï¼ˆå›ºå®šä¸º2ï¼Œè‹¥åº“ä¸è¶³å¯èƒ½å°äº2ï¼‰
            - items_per_batch: æ¯æ‰¹æ¡ç›®æ•°é‡ï¼ˆå³å…¥å‚ top_kï¼‰
            - results: åˆ—è¡¨ï¼Œæ¯é¡¹ä¸ºä¸€æ‰¹ï¼Œå« batch_rankã€relevance_scoreã€chunk_infoã€items
        
    ä½¿ç”¨ç¤ºä¾‹:
        - "æŸ¥æ‰¾è®¢å•ç›¸å…³çš„éœ€æ±‚"
        - "ç”¨æˆ·è¯„ä»·åŠŸèƒ½çš„ç¼ºé™·"
        - "é«˜ä¼˜å…ˆçº§çš„å¼€å‘ä»»åŠ¡"
    """
    try:
        result = await search_tapd_data(query, top_k)
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        error_result = {
            "status": "error", 
            "message": f"æœç´¢å¤±è´¥ï¼š{str(e)}"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
async def generate_fake_tapd_data(
    n_story_A: int = 300, 
    n_story_B: int = 200,
    n_bug_A: int = 400, 
    n_bug_B: int = 300,
    output_path: str = "local_data/msg_from_fetcher.json"
) -> str:
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„TAPDéœ€æ±‚å’Œç¼ºé™·æ•°æ®
    
    åŠŸèƒ½æè¿°:
        - ä½¿ç”¨Fakeråº“ç”Ÿæˆä¸­æ–‡æ¨¡æ‹Ÿæ•°æ®ï¼Œç”¨äºæµ‹è¯•å’Œå¼€å‘
        - åˆ›å»ºä¸¤ç§ç±»å‹çš„éœ€æ±‚ï¼šç®—æ³•ç­–ç•¥ç±»å’ŒåŠŸèƒ½å†³ç­–ç±»éœ€æ±‚
        - åˆ›å»ºä¸¤ç§ç±»å‹çš„ç¼ºé™·ï¼šç®€å•ç¼ºé™·å’ŒåŒ…å«è¯¦ç»†å¤ç°æ­¥éª¤çš„ç¼ºé™·
        - è‡ªåŠ¨æ·»åŠ ä¼˜å…ˆçº§ã€çŠ¶æ€ã€åˆ›å»ºæ—¶é—´ã€è´Ÿè´£äººã€æ ‡ç­¾ç­‰å…¬å…±å­—æ®µ
        - ç”Ÿæˆçš„æ•°æ®æ ¼å¼ä¸çœŸå®TAPDæ•°æ®ä¿æŒä¸€è‡´
        
    å‚æ•°:
        n_story_A (int): ç®—æ³•ç­–ç•¥ç±»éœ€æ±‚æ•°é‡ï¼Œé»˜è®¤300æ¡
        n_story_B (int): åŠŸèƒ½å†³ç­–ç±»éœ€æ±‚æ•°é‡ï¼Œé»˜è®¤200æ¡
        n_bug_A (int): ç®€å•ç¼ºé™·æ•°é‡ï¼Œé»˜è®¤400æ¡
        n_bug_B (int): è¯¦ç»†ç¼ºé™·æ•°é‡ï¼Œé»˜è®¤300æ¡
        output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º local_data/msg_from_fetcher.json

    è¿”å›:
        str: ç”Ÿæˆç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ç”Ÿæˆçš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        
    ä½¿ç”¨åœºæ™¯:
        - å¼€å‘å’Œæµ‹è¯•é˜¶æ®µç”Ÿæˆæµ‹è¯•æ•°æ®
        - éªŒè¯å‘é‡åŒ–å’Œæœç´¢åŠŸèƒ½
        - æ¼”ç¤ºå’ŒåŸ¹è®­åœºæ™¯
    """
    try:
        import os
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        dir_path = os.path.dirname(output_path)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)
        
        # è°ƒç”¨ç”Ÿæˆå‡½æ•°
        fake_generate(n_story_A, n_story_B, n_bug_A, n_bug_B, output_path)
        
        total_items = n_story_A + n_story_B + n_bug_A + n_bug_B
        result = {
            "status": "success", 
            "message": f"Successfully generated {total_items} TAPD items",
            "details": {
                "story_type_A": n_story_A,
                "story_type_B": n_story_B, 
                "bug_type_A": n_bug_A,
                "bug_type_B": n_bug_B,
                "total": total_items,
                "output_file": output_path
            }
        }
        # ä½¿ç”¨ASCIIå®‰å…¨æ¨¡å¼è¿”å›ç»“æœï¼Œé¿å…ç¼–ç é—®é¢˜
        return json.dumps(result, ensure_ascii=True, indent=2)
    except UnicodeEncodeError as e:
        error_result = {
            "status": "error",
            "message": f"Encoding error during data generation: {str(e)}",
            "suggestion": "Try using ASCII-safe file paths and avoid special characters"
        }
        return json.dumps(error_result, ensure_ascii=True, indent=2)
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"Failed to generate fake data: {str(e)}"
        }
        return json.dumps(error_result, ensure_ascii=True, indent=2)

@mcp.tool()
async def generate_tapd_overview(
    since: str = "2025-01-01",
    until: str = datetime.now().strftime("%Y-%m-%d"),
    max_total_tokens: int = 6000,
    use_local_data: bool = True,
    ack_mode: str = "ack_only",
    max_retries: int = 2,
    retry_backoff: float = 1.5,
    chunk_size: int = 0
) -> str:
    """ç”ŸæˆTAPDæ•°æ®çš„æ™ºèƒ½æ¦‚è§ˆå’Œæ‘˜è¦
    
    åŠŸèƒ½æè¿°:
        - ä½¿ç”¨ä¸Šä¸‹æ–‡ä¼˜åŒ–å™¨å¤„ç†å¤§å‹TAPDæ•°æ®é›†
        - åŸºäºtokenæ•°é‡è‡ªåŠ¨åˆ†å—æ•°æ®ï¼Œé¿å…tokenè¶…é™é—®é¢˜
        - é›†æˆåœ¨çº¿LLM APIè¿›è¡Œæ™ºèƒ½æ‘˜è¦
        - é€’å½’æ‘˜è¦ç”Ÿæˆï¼Œå°†å¤šä¸ªæ•°æ®å—çš„æ‘˜è¦åˆå¹¶ä¸ºæ€»ä½“æ¦‚è§ˆ
        - æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤ï¼Œè·å–æŒ‡å®šæœŸé—´çš„æ•°æ®æ‘˜è¦
        
    å‚æ•°:
        since (str): å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œé»˜è®¤ "2025-01-01"
        until (str): ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œé»˜è®¤ä¸ºå½“å‰ç³»ç»Ÿæ—¥æœŸ
        max_total_tokens (int): æœ€å¤§tokenæ•°é‡ï¼Œé»˜è®¤6000
        use_local_data (bool): æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ•°æ®ï¼Œé»˜è®¤Trueï¼ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼‰ï¼ŒFalseæ—¶ä»TAPD APIè·å–æœ€æ–°æ•°æ®
        
    è¿”å›:
        str: æ¦‚è§ˆç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«æ•°æ®ç»Ÿè®¡å’Œæ™ºèƒ½æ‘˜è¦
        
    æ³¨æ„äº‹é¡¹:
        - éœ€é…ç½®ç¯å¢ƒå˜é‡ SF_KEY (SiliconFlow) æˆ– DS_KEY (DeepSeek)
        - é¦–æ¬¡ä½¿ç”¨æ—¶éœ€è¦ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œç”¨äºè°ƒç”¨åœ¨çº¿LLM
        - å¤„ç†å¤§é‡æ•°æ®æ—¶å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        
    ä½¿ç”¨åœºæ™¯:
        - ç”Ÿæˆé¡¹ç›®è´¨é‡åˆ†ææŠ¥å‘Š
        - å¿«é€Ÿäº†è§£é¡¹ç›®æ•´ä½“æƒ…å†µ
        - ä¸ºç®¡ç†å±‚æä¾›æ•°æ®æ¦‚è§ˆ
    """
    try:
        # å¯¼å…¥å¿…è¦çš„å‡½æ•°
        from tapd_data_fetcher import (
            get_story_msg, get_bug_msg, 
            get_local_story_msg_filtered, get_local_bug_msg_filtered,
            get_story_msg_filtered, get_bug_msg_filtered
        )
        
        # æ ¹æ®å‚æ•°é€‰æ‹©æ•°æ®æºå¹¶ç›´æ¥è·å–ç­›é€‰åçš„æ•°æ®
        if use_local_data:
            print(f"[æœ¬åœ°æ•°æ®] ä½¿ç”¨æœ¬åœ°æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æï¼Œæ—¶é—´èŒƒå›´ï¼š{since} åˆ° {until}", file=sys.stderr, flush=True)
            stories_data = await get_local_story_msg_filtered(since, until)
            bugs_data = await get_local_bug_msg_filtered(since, until)
        else:
            print(f"[APIæ•°æ®] ä»TAPD APIè·å–æœ€æ–°æ•°æ®è¿›è¡Œåˆ†æï¼Œæ—¶é—´èŒƒå›´ï¼š{since} åˆ° {until}", file=sys.stderr, flush=True)
            stories_data = await get_story_msg_filtered(since, until)
            bugs_data = await get_bug_msg_filtered(since, until)
        
        print(f"[æ•°æ®åŠ è½½] æ•°æ®åŠ è½½å®Œæˆï¼š{len(stories_data)} æ¡éœ€æ±‚ï¼Œ{len(bugs_data)} æ¡ç¼ºé™·", file=sys.stderr, flush=True)
        
        # åŒ…è£…è·å–å‡½æ•°ä»¥é€‚é…context_optimizerçš„æ¥å£
        async def fetch_story(**params):
            # ç›´æ¥è¿”å›å·²ç­›é€‰çš„æ•°æ®ï¼Œæ— éœ€åˆ†é¡µå¤„ç†
            return stories_data
            
        async def fetch_bug(**params):
            # ç›´æ¥è¿”å›å·²ç­›é€‰çš„æ•°æ®ï¼Œæ— éœ€åˆ†é¡µå¤„ç†
            return bugs_data

        print(f"[AIåˆ†æ] å¼€å§‹è°ƒç”¨AIç”Ÿæˆæ™ºèƒ½æ¦‚è§ˆåˆ†æ...", file=sys.stderr, flush=True)
        print("[å¤„ç†ä¸­] æ­£åœ¨å¤„ç†æ•°æ®å¹¶ç”Ÿæˆè´¨é‡åˆ†ææŠ¥å‘Šï¼Œé¢„è®¡éœ€è¦10-20ç§’...", file=sys.stderr, flush=True)
        print(f"[å¯é ä¼ è¾“] ACKæ¨¡å¼: {ack_mode}ï¼Œé‡è¯•æ¬¡æ•°: {max_retries}ï¼Œå›é€€: {retry_backoff}ï¼Œåˆ†å—å¤§å°: {chunk_size or 'è‡ªåŠ¨'}", file=sys.stderr, flush=True)
        
        # è°ƒç”¨ä¸Šä¸‹æ–‡ä¼˜åŒ–å™¨
        overview = await build_overview(
            fetch_story=fetch_story,
            fetch_bug=fetch_bug,
            since=since,
            until=until,
            max_total_tokens=max_total_tokens,
            ack_mode=ack_mode,
            max_retries=max_retries,
            retry_backoff=retry_backoff,
            chunk_size=chunk_size
        )

        print("[åˆ†æå®Œæˆ] AIåˆ†æå®Œæˆï¼Œæ­£åœ¨æ•´ç†è¾“å‡ºç»“æœ...", file=sys.stderr, flush=True)
        
        # æ£€æŸ¥æ‘˜è¦æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
        summary_text = overview.get("summary_text", "")
        if "æ— æ³•ç”Ÿæˆæ™ºèƒ½æ‘˜è¦" in summary_text or "APIé…ç½®é”™è¯¯" in summary_text:
            # å¦‚æœæ‘˜è¦åŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œè¿”å›é”™è¯¯çŠ¶æ€
            suggestion = "è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ SF_KEY (SiliconFlow) æˆ– DS_KEY (DeepSeek) æ˜¯å¦å·²æ­£ç¡®è®¾ç½®"
            error_result = {
                "status": "error",
                "message": "æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå¤±è´¥",
                "details": summary_text,
                "suggestion": suggestion,
                "time_range": f"{since} è‡³ {until}",
                "total_stories": overview.get("total_stories", 0),
                "total_bugs": overview.get("total_bugs", 0),
                "chunks": overview.get("chunks", 0)
            }
            return json.dumps(error_result, ensure_ascii=False, indent=2)
        
        result = {
            "status": "success",
            "time_range": f"{since} è‡³ {until}",
            **overview
        }
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"ç”Ÿæˆæ¦‚è§ˆå¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®å’Œç½‘ç»œè¿æ¥"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
async def summarize_docx(docx_path: str, max_paragraphs: int = 5) -> str:
    """
    è¯»å– docx æ–‡æ¡£ï¼Œè¿”å›æ‰€æœ‰æ®µè½å†…å®¹å’Œæ‘˜è¦çš„ JSON æ•°æ®
    
    å‚æ•°ï¼š
        docx_path (str): .docx æ–‡ä»¶è·¯å¾„
        max_paragraphs (int): æ‘˜è¦æœ€å¤šåŒ…å«çš„æ®µè½æ•°ï¼Œé»˜è®¤5
    è¿”å›ï¼š
        str: JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«æ‰€æœ‰æ®µè½å’Œæ‘˜è¦
    """
    # å…¼å®¹ async è°ƒç”¨
    import asyncio
    loop = asyncio.get_event_loop() if asyncio.get_event_loop().is_running() else asyncio.new_event_loop()
    return await loop.run_in_executor(None, _summarize_docx, docx_path, max_paragraphs)

@mcp.tool()
async def analyze_word_frequency(
    min_frequency: int = 3,
    use_extended_fields: bool = True,
    data_file_path: str = "local_data/msg_from_fetcher.json"
) -> str:
    """
    åˆ†æTAPDæ•°æ®çš„è¯é¢‘åˆ†å¸ƒï¼Œç”Ÿæˆå…³é”®è¯è¯äº‘ç»Ÿè®¡
    
    åŠŸèƒ½æè¿°:
        - ä»TAPDæ•°æ®ä¸­æå–å…³é”®è¯å¹¶ç»Ÿè®¡è¯é¢‘
        - æ”¯æŒä¸­æ–‡åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤
        - ä¸ºæœç´¢åŠŸèƒ½æä¾›å‡†ç¡®çš„å…³é”®è¯å»ºè®®
        - ç”Ÿæˆè¯é¢‘åˆ†å¸ƒç»Ÿè®¡å’Œåˆ†ç±»å…³é”®è¯
        
    å‚æ•°:
        min_frequency (int): æœ€å°è¯é¢‘é˜ˆå€¼ï¼Œé»˜è®¤3
            - åªè¿”å›å‡ºç°æ¬¡æ•° >= min_frequency çš„è¯æ±‡
            - æ¨èå€¼: 3-10ï¼ˆæ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰
        use_extended_fields (bool): æ˜¯å¦ä½¿ç”¨æ‰©å±•å­—æ®µï¼Œé»˜è®¤True
            - True: åˆ†æ name, description, test_focus, label, acceptance, comment, status, priority, iteration_id
            - False: ä»…åˆ†æ name, description, test_focus, label
        data_file_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º local_data/msg_from_fetcher.json
            - æ”¯æŒè‡ªå®šä¹‰æ•°æ®æºè·¯å¾„
            
    è¿”å›:
        str: è¯é¢‘åˆ†æç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«:
            - é«˜é¢‘è¯ç»Ÿè®¡
            - é¢‘æ¬¡åˆ†å¸ƒ
            - æœç´¢å…³é”®è¯å»ºè®®
            - åˆ†ç±»å…³é”®è¯æ¨è
            
    ä½¿ç”¨åœºæ™¯:
        - ä¸º search_data æä¾›ç²¾å‡†æœç´¢å…³é”®è¯
        - ç”Ÿæˆé¡¹ç›®è¯äº‘å¯è§†åŒ–æ•°æ®
        - äº†è§£é¡¹ç›®é‡ç‚¹å…³æ³¨é¢†åŸŸå’Œå¸¸è§é—®é¢˜
        - ä¼˜åŒ–æœç´¢æŸ¥è¯¢çš„å‡†ç¡®æ€§
        
    æ³¨æ„äº‹é¡¹:
        - é¦–æ¬¡ä½¿ç”¨å‰è¯·ç¡®ä¿å·²è°ƒç”¨ get_tapd_data è·å–æ•°æ®
        - ä¸­æ–‡åˆ†è¯åŸºäºjiebaåº“ï¼Œé€‚åˆä¸­æ–‡é¡¹ç›®åˆ†æ
        - è‡ªåŠ¨è¿‡æ»¤å¸¸è§åœç”¨è¯ï¼Œä¸“æ³¨äºæœ‰æ„ä¹‰çš„å…³é”®è¯
    """
    try:
        result = await analyze_tapd_word_frequency(
            min_frequency=min_frequency,
            use_extended_fields=use_extended_fields,
            data_file_path=data_file_path
        )
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"è¯é¢‘åˆ†æå¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå»ºè®®å…ˆè°ƒç”¨ get_tapd_data å·¥å…·è·å–æ•°æ®"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
async def preprocess_tapd_description(
    data_file_path: str = "local_data/msg_from_fetcher.json",
    output_file_path: str = "local_data/msg_from_fetcher.json",
    use_api: bool = True,
    process_documents: bool = False,
    process_images: bool = False
) -> str:
    """
    é¢„å¤„ç†TAPDæ•°æ®çš„descriptionå­—æ®µï¼Œæ¸…ç†HTMLæ ·å¼å¹¶ä½¿ç”¨AIä¼˜åŒ–å†…å®¹
    
    åŠŸèƒ½æè¿°:
        - æ¸…ç†descriptionå­—æ®µä¸­çš„HTMLæ ·å¼ä¿¡æ¯ï¼ˆmarginã€paddingã€colorç­‰æ— ç”¨å±æ€§ï¼‰
        - ä¿ç•™æœ‰æ„ä¹‰çš„æ–‡å­—å†…å®¹ã€è¶…é“¾æ¥å’Œå›¾ç‰‡åœ°å€
        - ä½¿ç”¨DeepSeek APIå¯¹å†…å®¹è¿›è¡Œå‡†ç¡®å¤è¿°ï¼Œå‹ç¼©å†—ä½™ä¿¡æ¯
        - æå–è…¾è®¯æ–‡æ¡£é“¾æ¥å’Œå›¾ç‰‡è·¯å¾„ä¿¡æ¯
        - ä¸ºæœªæ¥çš„æ–‡æ¡£å†…å®¹æå–å’ŒOCRè¯†åˆ«é¢„ç•™æ¥å£
        
    å‚æ•°:
        data_file_path (str): è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤"local_data/msg_from_fetcher.json"
        output_file_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤"local_data/msg_from_fetcher.json"
        use_api (bool): æ˜¯å¦ä½¿ç”¨DeepSeek APIè¿›è¡Œå†…å®¹å¤è¿°ï¼Œé»˜è®¤True
        process_documents (bool): æ˜¯å¦å¤„ç†è…¾è®¯æ–‡æ¡£é“¾æ¥ï¼ˆé¢„ç•™åŠŸèƒ½ï¼‰ï¼Œé»˜è®¤False
        process_images (bool): æ˜¯å¦å¤„ç†å›¾ç‰‡å†…å®¹ï¼ˆé¢„ç•™åŠŸèƒ½ï¼‰ï¼Œé»˜è®¤False
        
    è¿”å›:
        str: å¤„ç†ç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯
        
    å¤„ç†æ•ˆæœ:
        - åŸå§‹HTMLé•¿åº¦é€šå¸¸å¯å‹ç¼©60-80%
        - ä¿ç•™æ‰€æœ‰å…³é”®ä¸šåŠ¡ä¿¡æ¯å’ŒæŠ€æœ¯ç»†èŠ‚
        - æå‡åç»­å‘é‡åŒ–å’Œæœç´¢çš„å‡†ç¡®æ€§
        - å‡å°‘tokenæ¶ˆè€—ï¼Œæé«˜AIå¤„ç†æ•ˆç‡
        
    ä½¿ç”¨åœºæ™¯:
        - è·å–TAPDæ•°æ®åçš„ç¬¬ä¸€æ­¥é¢„å¤„ç†
        - ä¸ºè¯é¢‘åˆ†æå’Œå‘é‡åŒ–å‡†å¤‡æ¸…æ´æ•°æ®
        - ä¼˜åŒ–AIåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆçš„è¾“å…¥è´¨é‡
        
    æ³¨æ„äº‹é¡¹:
        - éœ€è¦è®¾ç½®DS_KEYç¯å¢ƒå˜é‡ï¼ˆDeepSeek APIå¯†é’¥ï¼‰
        - å»ºè®®å…ˆä½¿ç”¨preview_description_cleaningé¢„è§ˆæ•ˆæœ
        - å¤„ç†å¤§é‡æ•°æ®æ—¶å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
    """
    try:
        result = await preprocess_description_field(
            data_file_path=data_file_path,
            output_file_path=output_file_path,
            use_api=use_api,
            process_documents=process_documents,
            process_images=process_images
        )
        return result
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ŒAPIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
def preview_tapd_description_cleaning(
    data_file_path: str = "local_data/msg_from_fetcher.json",
    item_count: int = 3
) -> str:
    """
    é¢„è§ˆTAPDæ•°æ®descriptionå­—æ®µæ¸…ç†æ•ˆæœï¼Œä¸å®é™…ä¿®æ”¹æ•°æ®
    
    åŠŸèƒ½æè¿°:
        - å±•ç¤ºHTMLæ ·å¼æ¸…ç†å‰åçš„å¯¹æ¯”æ•ˆæœ
        - ç»Ÿè®¡å‹ç¼©æ¯”ä¾‹å’Œæå–çš„é“¾æ¥ã€å›¾ç‰‡ä¿¡æ¯
        - å¸®åŠ©ç”¨æˆ·äº†è§£é¢„å¤„ç†æ•ˆæœå’Œå†³å®šå‚æ•°è®¾ç½®
        - ä¸è°ƒç”¨APIï¼Œä»…å±•ç¤ºæ ·å¼æ¸…ç†æ•ˆæœ
        
    å‚æ•°:
        data_file_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤"local_data/msg_from_fetcher.json"
        item_count (int): é¢„è§ˆçš„æ¡ç›®æ•°é‡ï¼Œé»˜è®¤3æ¡
        
    è¿”å›:
        str: é¢„è§ˆç»“æœçš„JSONå­—ç¬¦ä¸²
        
    é¢„è§ˆä¿¡æ¯åŒ…æ‹¬:
        - åŸå§‹å†…å®¹å’Œæ¸…ç†åå†…å®¹çš„é•¿åº¦å¯¹æ¯”
        - å†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰
        - æå–çš„æ–‡æ¡£é“¾æ¥å’Œå›¾ç‰‡è·¯å¾„åˆ—è¡¨
        - æ¯ä¸ªæ¡ç›®çš„åŸºæœ¬ä¿¡æ¯ï¼ˆIDã€æ ‡é¢˜ç­‰ï¼‰
        
    ä½¿ç”¨åœºæ™¯:
        - åœ¨æ­£å¼é¢„å¤„ç†å‰è¯„ä¼°æ•ˆæœ
        - è°ƒè¯•å’Œä¼˜åŒ–æ¸…ç†è§„åˆ™
        - äº†è§£æ•°æ®è´¨é‡å’Œå¤æ‚åº¦
        
    æ³¨æ„äº‹é¡¹:
        - æ­¤å·¥å…·ä¸ä¼šä¿®æ”¹åŸå§‹æ•°æ®
        - ä¸éœ€è¦APIå¯†é’¥ï¼Œå¯å®‰å…¨ä½¿ç”¨
        - å»ºè®®åœ¨å¤§æ‰¹é‡å¤„ç†å‰å…ˆé¢„è§ˆ
    """
    try:
        result = preview_description_cleaning(
            data_file_path=data_file_path,
            item_count=item_count
        )
        return result
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"é¢„è§ˆå¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
async def enhance_tapd_with_knowledge(
    tapd_file: str = "local_data/msg_from_fetcher.json",
    testcase_file: Optional[str] = None
) -> str:
    """
    åˆ†æTAPDæ•°æ®å¹¶æ„å»ºå†å²éœ€æ±‚çŸ¥è¯†åº“
    
    åŠŸèƒ½æè¿°:
        - ä»TAPDæ•°æ®ä¸­åˆ†æéœ€æ±‚ä¿¡æ¯ï¼Œæ„å»ºç‹¬ç«‹çš„çŸ¥è¯†åº“é…ç½®æ–‡ä»¶
        - ä¸ºæ¯ä¸ªéœ€æ±‚åˆ†æåŠŸèƒ½ç±»å‹ã€æµ‹è¯•ç”¨ä¾‹å»ºè®®ã€å…³é”®è¯ç­‰
        - ä»…è¯»å–åŸå§‹æ•°æ®æ–‡ä»¶ï¼Œä¸ä¿®æ”¹åŸå§‹TAPDæ•°æ®
        - çŸ¥è¯†åº“ä¿¡æ¯ä¿å­˜åˆ° config/knowledge_base_config.json
        - é‡‡ç”¨ä¸test_case_require_list_knowledge_base.pyç›¸åŒçš„æ•°æ®ç®¡ç†æ–¹å¼
        
    å‚æ•°:
        tapd_file (str): TAPDæ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤"local_data/msg_from_fetcher.json"
        testcase_file (Optional[str]): æµ‹è¯•ç”¨ä¾‹Excelæ–‡ä»¶è·¯å¾„ï¼Œå¯é€‰
        
    è¿”å›:
        str: çŸ¥è¯†åº“åˆ†æç»“æœçš„JSONå­—ç¬¦ä¸²
        
    ä½¿ç”¨åœºæ™¯:
        - æ„å»ºé¡¹ç›®éœ€æ±‚çŸ¥è¯†åº“ï¼Œä¸ºåç»­åˆ†ææä¾›å‚è€ƒ
        - åˆ†æéœ€æ±‚åŠŸèƒ½ç±»å‹åˆ†å¸ƒå’Œå…³é”®è¯æ˜ å°„
        - ä¸ºæµ‹è¯•ç”¨ä¾‹è®¾è®¡æä¾›å†å²éœ€æ±‚å‚è€ƒ
        - æ”¯æŒéœ€æ±‚è¶‹åŠ¿åˆ†æå’Œæ¨¡å¼è¯†åˆ«
        
    æ³¨æ„äº‹é¡¹:
        - åŸå§‹TAPDæ•°æ®æ–‡ä»¶ä¿æŒä¸å˜
        - çŸ¥è¯†åº“é…ç½®æ–‡ä»¶ç‹¬ç«‹å­˜å‚¨åœ¨configç›®å½•
        - å¯é‡å¤æ‰§è¡Œä»¥æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯
    """
    try:
        result = enhance_tapd_data_with_knowledge(tapd_file, testcase_file)
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"æ•°æ®å¢å¼ºå¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥TAPDæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå»ºè®®å…ˆè°ƒç”¨ get_tapd_data å·¥å…·è·å–æ•°æ®"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def analyze_time_trends(
    data_type: Literal['story', 'bug'] = "story",
    chart_type: Literal['count', 'priority', 'status'] = "count",
    time_field: str = "created",
    since: Optional[str] = None,
    until: Optional[str] = None,
    data_file_path: str = "local_data/msg_from_fetcher.json"
) -> str:
    """
    åˆ†æTAPDæ•°æ®çš„æ—¶é—´è¶‹åŠ¿å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    
    åŠŸèƒ½æè¿°:
        - æ”¯æŒåˆ†æéœ€æ±‚å’Œç¼ºé™·æ•°æ®çš„æ—¶é—´è¶‹åŠ¿
        - ç”Ÿæˆä¸‰ç§ç±»å‹çš„å›¾è¡¨ï¼šæ€»æ•°è¶‹åŠ¿ã€ä¼˜å…ˆçº§åˆ†å¸ƒã€çŠ¶æ€åˆ†å¸ƒ
        - æ”¯æŒè‡ªå®šä¹‰æ—¶é—´èŒƒå›´å’Œç»Ÿè®¡å­—æ®µ
        - è‡ªåŠ¨åˆ›å»ºtime_trendç›®å½•å­˜å‚¨ç”Ÿæˆçš„å›¾è¡¨
        
    å‚æ•°:
        data_type (str): æ•°æ®ç±»å‹ï¼Œå¯é€‰ 'story'(éœ€æ±‚) æˆ– 'bug'(ç¼ºé™·)ï¼Œé»˜è®¤ 'story'
        chart_type (str): å›¾è¡¨ç±»å‹ï¼Œå¯é€‰ 'count'(æ€»æ•°)ã€'priority'(ä¼˜å…ˆçº§)ã€'status'(çŠ¶æ€)ï¼Œé»˜è®¤ 'count'
        time_field (str): æ—¶é—´å­—æ®µï¼Œé»˜è®¤ä¸º 'created'(åˆ›å»ºæ—¶é—´)ï¼Œå¯é€‰ 'modified'(ä¿®æ”¹æ—¶é—´)
        since (str): å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ YYYY-MM-DDï¼Œé»˜è®¤None(å…¨éƒ¨æ—¶é—´)
        until (str): ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ YYYY-MM-DDï¼Œé»˜è®¤None(å…¨éƒ¨æ—¶é—´)
        data_file_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ "local_data/msg_from_fetcher.json"
        
    è¿”å›:
        str: åˆ†æç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œå›¾è¡¨è·¯å¾„
        
    ä½¿ç”¨åœºæ™¯:
        - ç›‘æ§é¡¹ç›®éœ€æ±‚å’Œç¼ºé™·çš„æ•°é‡å˜åŒ–è¶‹åŠ¿
        - åˆ†æé«˜ä¼˜å…ˆçº§ä»»åŠ¡çš„åˆ†å¸ƒæƒ…å†µ
        - è·Ÿè¸ªä»»åŠ¡å®ŒæˆçŠ¶æ€çš„æ—¶é—´å˜åŒ–
        - ç”Ÿæˆé¡¹ç›®è´¨é‡æŠ¥å‘Šçš„å¯è§†åŒ–å›¾è¡¨
        
    æ³¨æ„äº‹é¡¹:
        - éœ€è¦å…ˆè°ƒç”¨ get_tapd_data è·å–æ•°æ®
        - å›¾è¡¨ä¿å­˜åˆ° local_data/time_trend ç›®å½•
        - æ”¯æŒä¸­è‹±æ–‡æ˜¾ç¤ºï¼Œè‡ªåŠ¨å¤„ç†æ—¶é—´æ ¼å¼
    """
    try:
        result = await analyze_trends(
            data_type=data_type,
            chart_type=chart_type,
            time_field=time_field,
            since=since,
            until=until,
            data_file_path=data_file_path
        )
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"æ—¶é—´è¶‹åŠ¿åˆ†æå¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œæ—¶é—´æ ¼å¼æ˜¯å¦æ­£ç¡®(YYYY-MM-DD)"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def precise_search_tapd_data(
    search_value: str,
    search_field: Optional[str] = None,
    data_type: Literal['stories', 'bugs', 'both'] = "both",
    exact_match: bool = False,
    case_sensitive: bool = False
) -> str:
    """
    TAPDæ•°æ®ç²¾ç¡®æœç´¢å·¥å…·
    
    åŠŸèƒ½æè¿°:
        - å¯¹TAPDéœ€æ±‚å’Œç¼ºé™·æ•°æ®è¿›è¡Œç²¾ç¡®å­—æ®µæœç´¢
        - æ”¯æŒå…¨å­—æ®µæœç´¢æˆ–æŒ‡å®šå­—æ®µæœç´¢
        - æä¾›ç²¾ç¡®åŒ¹é…å’Œæ¨¡ç³ŠåŒ¹é…æ¨¡å¼
        - å¯é€‰æ‹©æœç´¢éœ€æ±‚ã€ç¼ºé™·æˆ–ä¸¤è€…
        
    å‚æ•°:
        search_value (str): æœç´¢å€¼ï¼Œè¦æŸ¥æ‰¾çš„å†…å®¹
        search_field (str, optional): æœç´¢å­—æ®µï¼ŒNoneè¡¨ç¤ºæœç´¢æ‰€æœ‰å­—æ®µã€‚
                                     éœ€æ±‚å­—æ®µ: id, name, description, creator, priority, priority_label, statusç­‰
                                     ç¼ºé™·å­—æ®µ: id, title, description, reporter, priority, severity, statusç­‰
        data_type (str): æ•°æ®ç±»å‹ï¼Œå¯é€‰ 'stories'(éœ€æ±‚)ã€'bugs'(ç¼ºé™·)ã€'both'(ä¸¤è€…)ï¼Œé»˜è®¤ 'both'
        exact_match (bool): æ˜¯å¦ç²¾ç¡®åŒ¹é…ï¼ŒTrue=å®Œå…¨ç›¸ç­‰ï¼ŒFalse=åŒ…å«åŒ¹é…ï¼Œé»˜è®¤ False
        case_sensitive (bool): æ˜¯å¦åŒºåˆ†å¤§å°å†™ï¼Œé»˜è®¤ False
        
    è¿”å›:
        str: æœç´¢ç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«åŒ¹é…çš„æ•°æ®é¡¹å’Œç»Ÿè®¡æ‘˜è¦
        
    ä½¿ç”¨åœºæ™¯:
        - æ ¹æ®IDç²¾ç¡®æŸ¥æ‰¾ç‰¹å®šéœ€æ±‚æˆ–ç¼ºé™·
        - æœç´¢åŒ…å«ç‰¹å®šå…³é”®è¯çš„éœ€æ±‚æè¿°
        - æŸ¥æ‰¾æŸä¸ªåˆ›å»ºè€…çš„æ‰€æœ‰ä»»åŠ¡
        - æ£€ç´¢ç‰¹å®šçŠ¶æ€çš„å·¥ä½œé¡¹
        
    ç¤ºä¾‹:
        - æœç´¢ID: search_value="1148591566001000001", search_field="id"
        - æœç´¢åˆ›å»ºè€…: search_value="å¼ å‡¯æ™¨", search_field="creator"
        - æ¨¡ç³Šæœç´¢æ ‡é¢˜: search_value="ç™»å½•", search_field="name", exact_match=False
        - æœç´¢æ‰€æœ‰å­—æ®µ: search_value="å‰ç«¯å¼€å‘", search_field=None
    """
    try:
        result = precise_search(
            search_value=search_value,
            search_field=search_field,
            data_type=data_type,
            exact_match=exact_match,
            case_sensitive=case_sensitive
        )
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"ç²¾ç¡®æœç´¢å¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æœç´¢å‚æ•°æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def search_tapd_by_priority(
    priority_filter: Literal['high', 'medium', 'low', 'all', 'urgent', 'Nice To Have', 'Low', 'Middle', 'High'] = "high",
    data_type: Literal['stories', 'bugs', 'both'] = "both"
) -> str:
    """
    æŒ‰ä¼˜å…ˆçº§æœç´¢TAPDæ•°æ®
    
    åŠŸèƒ½æè¿°:
        - æ ¹æ®ä¼˜å…ˆçº§ç­›é€‰TAPDéœ€æ±‚å’Œç¼ºé™·æ•°æ®
        - æ”¯æŒé«˜ä¸­ä½ä¼˜å…ˆçº§é¢„è®¾è¿‡æ»¤å™¨
        - æ”¯æŒå…·ä½“ä¼˜å…ˆçº§æ ‡ç­¾æœç´¢
        - é»˜è®¤æŸ¥æ‰¾é«˜ä¼˜å…ˆçº§æ•°æ®
        
    å‚æ•°:
        priority_filter (str): ä¼˜å…ˆçº§è¿‡æ»¤å™¨ï¼Œå¯é€‰å€¼ï¼š
                              é€šç”¨: 'high'(é«˜)ã€'medium'(ä¸­)ã€'low'(ä½)ã€'all'(å…¨éƒ¨)
                              éœ€æ±‚ä¸“ç”¨: 'Nice To Have'ã€'Low'ã€'Middle'ã€'High'
                              ç¼ºé™·ä¸“ç”¨: 'urgent'(ç´§æ€¥)
        data_type (str): æ•°æ®ç±»å‹ï¼Œå¯é€‰ 'stories'(éœ€æ±‚)ã€'bugs'(ç¼ºé™·)ã€'both'(ä¸¤è€…)ï¼Œé»˜è®¤ 'both'
        
    è¿”å›:
        str: æœç´¢ç»“æœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ç¬¦åˆä¼˜å…ˆçº§æ¡ä»¶çš„æ•°æ®é¡¹å’Œç»Ÿè®¡æ‘˜è¦
        
    ä¼˜å…ˆçº§è¯´æ˜:
        éœ€æ±‚ä¼˜å…ˆçº§ (æ•°å­—è¶Šå¤§è¶Šç´§æ€¥):
        - 1: Nice To Have
        - 2: Low  
        - 3: Middle (ä¸­ä¼˜å…ˆçº§)
        - 4: High (é«˜ä¼˜å…ˆçº§)
        
        ç¼ºé™·ä¼˜å…ˆçº§:
        - insignificant: å¾®ä¸è¶³é“
        - low: ä½
        - medium: ä¸­
        - high: é«˜
        - urgent: ç´§æ€¥
        
    ä½¿ç”¨åœºæ™¯:
        - å¿«é€ŸæŸ¥çœ‹æ‰€æœ‰é«˜ä¼˜å…ˆçº§ä»»åŠ¡
        - ç»Ÿè®¡ä¸åŒä¼˜å…ˆçº§ä»»åŠ¡çš„åˆ†å¸ƒ
        - ç­›é€‰ç´§æ€¥éœ€è¦å¤„ç†çš„ç¼ºé™·
        - ç”Ÿæˆä¼˜å…ˆçº§æŠ¥å‘Š
    """
    try:
        result = search_by_priority(
            priority_filter=priority_filter,
            data_type=data_type
        )
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"ä¼˜å…ˆçº§æœç´¢å¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥ä¼˜å…ˆçº§å‚æ•°æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def get_tapd_data_statistics(
    data_type: Literal['stories', 'bugs', 'both'] = "both"
) -> str:
    """
    è·å–TAPDæ•°æ®ç»Ÿè®¡ä¿¡æ¯
    
    åŠŸèƒ½æè¿°:
        - æä¾›TAPDæ•°æ®çš„å…¨é¢ç»Ÿè®¡åˆ†æ
        - åŒ…å«æ•°é‡åˆ†å¸ƒã€ä¼˜å…ˆçº§åˆ†å¸ƒã€çŠ¶æ€åˆ†å¸ƒç­‰
        - æ”¯æŒéœ€æ±‚å’Œç¼ºé™·çš„ç‹¬ç«‹ç»Ÿè®¡
        - ç”Ÿæˆæ•°æ®æ¦‚è§ˆæŠ¥å‘Š
        
    å‚æ•°:
        data_type (str): æ•°æ®ç±»å‹ï¼Œå¯é€‰ 'stories'(éœ€æ±‚)ã€'bugs'(ç¼ºé™·)ã€'both'(ä¸¤è€…)ï¼Œé»˜è®¤ 'both'
        
    è¿”å›:
        str: ç»Ÿè®¡ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«è¯¦ç»†çš„åˆ†å¸ƒæ•°æ®å’Œæ±‡æ€»ä¿¡æ¯
        
    ç»Ÿè®¡å†…å®¹:
        éœ€æ±‚ç»Ÿè®¡:
        - æ€»æ•°é‡ã€ä¼˜å…ˆçº§åˆ†å¸ƒã€çŠ¶æ€åˆ†å¸ƒ
        - åˆ›å»ºè€…åˆ†å¸ƒã€æœ€è¿‘é¡¹ç›®æ•°ã€å·²å®Œæˆé¡¹ç›®æ•°
        - é«˜ä¼˜å…ˆçº§é¡¹ç›®æ•°é‡
        
        ç¼ºé™·ç»Ÿè®¡:
        - æ€»æ•°é‡ã€ä¼˜å…ˆçº§åˆ†å¸ƒã€ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        - çŠ¶æ€åˆ†å¸ƒã€æŠ¥å‘Šè€…åˆ†å¸ƒã€æœ€è¿‘ç¼ºé™·æ•°
        - å·²è§£å†³ç¼ºé™·æ•°ã€é«˜ä¼˜å…ˆçº§ç¼ºé™·æ•°
        
    ä½¿ç”¨åœºæ™¯:
        - ç”Ÿæˆé¡¹ç›®æ•°æ®æ¦‚è§ˆæŠ¥å‘Š
        - äº†è§£å·¥ä½œè´Ÿè½½åˆ†å¸ƒ
        - è¯„ä¼°é¡¹ç›®è´¨é‡çŠ¶å†µ
        - ä¸ºç®¡ç†å†³ç­–æä¾›æ•°æ®æ”¯æŒ
    """
    try:
        result = get_tapd_statistics(data_type=data_type)
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥ï¼š{str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå»ºè®®å…ˆè°ƒç”¨ get_tapd_data å·¥å…·è·å–æ•°æ®"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    # åœ¨å¯åŠ¨ MCP æœåŠ¡å™¨ä¹‹å‰æ¢å¤ stdoutï¼Œå› ä¸º MCP éœ€è¦é€šè¿‡ stdout è¿›è¡Œ JSON-RPC é€šä¿¡
    sys.stdout = _original_stdout

    # æ¨¡å‹é¢„çƒ­ - åœ¨åå°å¼‚æ­¥åŠ è½½æ¨¡å‹ä»¥é¿å…å·¥å…·è°ƒç”¨æ—¶é˜»å¡
    async def warm_up_models():
        """é¢„çƒ­æ¨¡å‹ï¼Œé¿å…åœ¨MCP Inspectorä¸­é¦–æ¬¡è°ƒç”¨æ—¶è¶…æ—¶"""
        try:
            print("ğŸ”¥ å¼€å§‹é¢„çƒ­å‘é‡åŒ–æ¨¡å‹...", file=sys.stderr, flush=True)
            from mcp_tools.common_utils import get_model_manager
            model_manager = get_model_manager()
            success = await model_manager.warm_up_model("paraphrase-MiniLM-L6-v2")
            if success:
                print("ğŸ‰ æ¨¡å‹é¢„çƒ­å®Œæˆï¼ŒMCP Inspectorå¯æµç•…ä½¿ç”¨å‘é‡åŒ–åŠŸèƒ½", file=sys.stderr, flush=True)
            else:
                print("âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥ï¼Œé¦–æ¬¡ä½¿ç”¨æ—¶å¯èƒ½è¾ƒæ…¢", file=sys.stderr, flush=True)
        except Exception as e:
            print(f"âŒ æ¨¡å‹é¢„çƒ­å¼‚å¸¸: {e}", file=sys.stderr, flush=True)
    
    # å¯åŠ¨é¢„çƒ­ä»»åŠ¡
    try:
        import asyncio
        print("ğŸš€ å¯åŠ¨MCPæœåŠ¡å™¨...", file=sys.stderr, flush=True)
        
        # åˆ›å»ºäº‹ä»¶å¾ªç¯å¹¶é¢„çƒ­æ¨¡å‹
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(warm_up_models())
        loop.close()
        
    except Exception as e:
        print(f"âš ï¸ é¢„çƒ­è¿‡ç¨‹å‡ºç°é—®é¢˜ï¼Œç»§ç»­å¯åŠ¨æœåŠ¡å™¨: {e}", file=sys.stderr, flush=True)

    # å¯åŠ¨MCPæœåŠ¡å™¨ï¼ˆä½¿ç”¨æ ‡å‡†è¾“å…¥è¾“å‡ºä¼ è¾“ï¼‰
    mcp.run(transport='stdio')