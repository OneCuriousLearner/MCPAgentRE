"""
AI æµ‹è¯•ç”¨ä¾‹è¯„ä¼°å™¨

æœ¬è„šæœ¬æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºä» Excel æ–‡ä»¶ä¸­è¯»å–æµ‹è¯•ç”¨ä¾‹ï¼Œ
åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„æ”¹è¿›å»ºè®®ã€‚
å®ƒé›†æˆäº†åŠ¨æ€tokenç®¡ç†ã€è‡ªå®šä¹‰è¯„ä¼°è§„åˆ™å’Œéœ€æ±‚å•çŸ¥è¯†åº“ï¼Œä»¥å®ç°é«˜æ•ˆã€å‡†ç¡®çš„è¯„ä¼°æµç¨‹ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1.  **æ•°æ®å¤„ç†**ï¼šä» Excel æ–‡ä»¶è¯»å–æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„ JSON æ ¼å¼ã€‚
2.  **Tokenç®¡ç†**ï¼šå†…ç½®ç²¾ç¡®çš„ Token è®¡æ•°å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨ `transformers` åº“ï¼‰ï¼Œå¹¶æ ¹æ®åŠ¨æ€è®¡ç®—çš„
    æç¤ºè¯é•¿åº¦è‡ªåŠ¨å°†å¤§é‡æµ‹è¯•ç”¨ä¾‹åˆ†å‰²æˆé€‚åˆæ¨¡å‹ä¸Šä¸‹æ–‡çª—å£çš„æ‰¹æ¬¡ã€‚
3.  **åŠ¨æ€æç¤ºè¯**ï¼šåŸºäº `test_case_rules_customer.py` ä¸­çš„è‡ªå®šä¹‰è§„åˆ™ï¼ˆå¦‚æ ‡é¢˜é•¿åº¦ã€æ­¥éª¤æ•°ï¼‰
    æ„å»ºè¯„ä¼°æç¤ºè¯ï¼Œç¡®ä¿è¯„ä¼°æ ‡å‡†çš„ä¸€è‡´æ€§å’Œçµæ´»æ€§ã€‚
4.  **çŸ¥è¯†åº“é›†æˆ**ï¼šå…³è” `test_case_require_list_knowledge_base.py` ä¸­çš„éœ€æ±‚å•çŸ¥è¯†åº“ï¼Œ
    åœ¨è¯„ä¼°æ—¶ä¸º LLM æä¾›ç›¸å…³çš„éœ€æ±‚èƒŒæ™¯ä¿¡æ¯ï¼Œæé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
5.  **å¼‚æ­¥è¯„ä¼°**ï¼šä½¿ç”¨ `aiohttp` è¿›è¡Œå¼‚æ­¥ API è°ƒç”¨ï¼Œå¹¶è¡Œå¤„ç†å¤šä¸ªè¯„ä¼°è¯·æ±‚ï¼Œæé«˜å¤„ç†æ•ˆç‡ã€‚
6.  **ç»“æœè§£æ**ï¼šèƒ½å¤Ÿè§£æ LLM è¿”å›çš„ Markdown è¡¨æ ¼æ ¼å¼çš„è¯„ä¼°ç»“æœï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„
    JSON æ•°æ®ï¼Œä¾¿äºåç»­åˆ†æå’Œå­˜å‚¨ã€‚

ç±»ä¸æ¨¡å—è¯´æ˜ï¼š
- `TokenCounter`: è´Ÿè´£è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡ã€‚ä¼˜å…ˆä½¿ç”¨æœ¬åœ°çš„ `transformers` tokenizer
  è¿›è¡Œç²¾ç¡®è®¡ç®—ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ”¹è¿›çš„é¢„ä¼°æ¨¡å¼ã€‚
- `TestCaseProcessor`: å¤„ç†æµ‹è¯•ç”¨ä¾‹æ•°æ®ã€‚ä¸»è¦åŠŸèƒ½æ˜¯å°† Excel æ–‡ä»¶è½¬æ¢ä¸º JSON æ ¼å¼ï¼Œ
  å¹¶è¿›è¡Œå¿…è¦çš„å­—æ®µæ˜ å°„å’Œæ•°æ®æ¸…ç†ã€‚
- `TestCaseEvaluator`: æ ¸å¿ƒè¯„ä¼°å™¨ç±»ã€‚
    - `__init__`: åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ŒåŠ è½½è§„åˆ™é…ç½®å’Œéœ€æ±‚å•çŸ¥è¯†åº“ï¼Œå¹¶æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶è®¡ç®—
      token åˆ†é…ç­–ç•¥ã€‚
    - `_build_dynamic_prompt_template`: æ ¹æ®è§„åˆ™åŠ¨æ€æ„å»ºå‘é€ç»™ LLM çš„æç¤ºè¯æ¨¡æ¿ã€‚
    - `estimate_batch_tokens`: ä¼°ç®—ä¸€ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•ç”¨ä¾‹åœ¨ç»„åˆæˆå•ä¸ªè¯·æ±‚åæ‰€éœ€çš„ token æ€»é‡ã€‚
    - `split_test_cases_by_tokens`: æ ¹æ® token é˜ˆå€¼å°†æ‰€æœ‰ç”¨ä¾‹åˆ†å‰²æˆå¤šä¸ªæ‰¹æ¬¡ã€‚
    - `evaluate_batch`: å¯¹å•ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œ AI è¯„ä¼°ï¼Œå‘é€å¼‚æ­¥è¯·æ±‚å¹¶è·å–ç»“æœã€‚
    - `parse_evaluation_result`: è§£æ AI è¿”å›çš„ Markdown æ ¼å¼çš„è¯„ä¼°ç»“æœã€‚
    - `evaluate_test_cases`: ç¼–æ’æ•´ä¸ªè¯„ä¼°æµç¨‹ï¼Œä»æ‰¹æ¬¡åˆ†å‰²åˆ°ç»“æœæ±‡æ€»ã€‚

å¤„ç†æµç¨‹ï¼š
1.  `main_process` å‡½æ•°å¯åŠ¨å¤„ç†æµç¨‹ã€‚
2.  `TestCaseProcessor` è¯»å– Excel æ–‡ä»¶å¹¶è½¬æ¢ä¸º JSONã€‚
3.  `TestCaseEvaluator` åŠ è½½æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ã€‚
4.  `split_test_cases_by_tokens` æ–¹æ³•æ ¹æ® `token_threshold` å°†ç”¨ä¾‹åˆ†å‰²æˆå¤šä¸ªæ‰¹æ¬¡ã€‚
5.  å¯¹äºæ¯ä¸ªæ‰¹æ¬¡ï¼Œ`evaluate_batch` æ–¹æ³•æ„å»ºä¸€ä¸ªåŒ…å«è¯¥æ‰¹æ¬¡æ‰€æœ‰ç”¨ä¾‹çš„æç¤ºè¯ï¼Œå¹¶å¼‚æ­¥è°ƒç”¨
    LLM APIã€‚
6.  `parse_evaluation_result` æ–¹æ³•è§£æè¿”å›çš„ Markdown è¡¨æ ¼ï¼Œæå–æ¯ä¸ªç”¨ä¾‹çš„è¯„åˆ†å’Œå»ºè®®ã€‚
7.  æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆåï¼Œç»“æœè¢«æ±‡æ€»å¹¶ä¿å­˜åˆ° `Proceed_TestCase_...json` æ–‡ä»¶ä¸­ã€‚
"""

import os
import re
import json
import asyncio
import aiohttp
import pandas as pd
import sys
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œmcp_toolsç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
mcp_tools_path = project_root / "mcp_tools"

if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(mcp_tools_path) not in sys.path:
    sys.path.insert(0, str(mcp_tools_path))

# å¯¼å…¥å…¬å…±å·¥å…·
from common_utils import get_config, get_api_manager, get_file_manager
from test_case_rules_customer import get_test_case_rules
from test_case_require_list_knowledge_base import RequirementKnowledgeBase


class TokenCounter:
    """Tokenè®¡æ•°å™¨ - æ”¯æŒtransformers tokenizerå’Œæ”¹è¿›çš„é¢„ä¼°æ¨¡å¼"""
    
    def __init__(self):
        self.config = get_config()
        self.tokenizer = None
        self._try_load_tokenizer()
    
    def _try_load_tokenizer(self):
        """å°è¯•åŠ è½½DeepSeek tokenizer"""
        try:
            # å°è¯•ä½¿ç”¨transformersåº“åŠ è½½tokenizer
            import transformers
            
            tokenizer_path = self.config.models_path / "deepseek_v3_tokenizer" / "deepseek_v3_tokenizer"
            
            if tokenizer_path.exists():
                self.tokenizer = transformers.AutoTokenizer.from_pretrained(
                    str(tokenizer_path), 
                    trust_remote_code=True
                )
                print("å·²åŠ è½½DeepSeek tokenizerï¼ˆtransformersï¼‰ï¼Œå°†ä½¿ç”¨ç²¾ç¡®tokenè®¡æ•°")
                return
            else:
                print(f"DeepSeek tokenizerè·¯å¾„ä¸å­˜åœ¨: {tokenizer_path}")
                
        except ImportError as e:
            print(f"transformersåº“æœªå®‰è£…: {e}")
        except Exception as e:
            print(f"åŠ è½½DeepSeek tokenizerå¤±è´¥: {e}")
        
        print("å°†ä½¿ç”¨æ”¹è¿›çš„é¢„ä¼°æ¨¡å¼è¿›è¡Œtokenè®¡æ•°")
        self.tokenizer = None
    
    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        å‚æ•°:
            text: è¦è®¡ç®—çš„æ–‡æœ¬
            
        è¿”å›:
            tokenæ•°é‡
        """
        if self.tokenizer:
            try:
                tokens = self.tokenizer.encode(text)
                return len(tokens)
            except Exception as e:
                print(f"ä½¿ç”¨tokenizerè®¡æ•°å¤±è´¥: {e}ï¼Œè½¬ä¸ºé¢„ä¼°æ¨¡å¼")
        
        # æ”¹è¿›çš„é¢„ä¼°æ¨¡å¼ï¼šåŸºäºæµ‹è¯•ç»“æœä¼˜åŒ–å‚æ•°
        # æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼š
        # - æ ·æœ¬æ–‡æœ¬å¹³å‡æ¯”ç‡: 0.98 (é¢„ä¼°vså®é™…)
        # - çœŸå®ç”¨ä¾‹å¹³å‡æ¯”ç‡: 0.91 (é¢„ä¼°vså®é™…)
        # - é¢„ä¼°æ¨¡å¼æ€»ä½“åä½çº¦10%ï¼Œéœ€è¦è°ƒæ•´ç³»æ•°
        
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        digits = len(re.findall(r'[0-9]', text))
        punctuation = len(re.findall(r'[^\w\s]', text))
        spaces = len(re.findall(r'\s', text))
        other_chars = len(text) - chinese_chars - english_chars - digits - punctuation - spaces
        
        # åŸºäºæµ‹è¯•ç»“æœè°ƒæ•´çš„ç³»æ•°
        estimated_tokens = int(
            chinese_chars * 0.7 +      # ä¸­æ–‡å­—ç¬¦ï¼ˆä»0.6è°ƒæ•´åˆ°0.7ï¼‰
            english_chars * 0.35 +     # è‹±æ–‡å­—ç¬¦ï¼ˆä»0.3è°ƒæ•´åˆ°0.35ï¼‰
            digits * 0.3 +             # æ•°å­—
            punctuation * 0.4 +        # æ ‡ç‚¹ç¬¦å·
            spaces * 0.1 +             # ç©ºæ ¼
            other_chars * 0.4          # å…¶ä»–å­—ç¬¦
        )
        
        # æ·»åŠ 10%çš„ä¿®æ­£ç³»æ•°ï¼Œå› ä¸ºæµ‹è¯•æ˜¾ç¤ºé¢„ä¼°åä½
        estimated_tokens = int(estimated_tokens * 1.1)
        
        return estimated_tokens


class TestCaseProcessor:
    """Excelæµ‹è¯•ç”¨ä¾‹å¤„ç†å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.file_manager = get_file_manager()
        
    def excel_to_json(self, excel_file_path: str, json_file_path: str) -> List[Dict[str, Any]]:
        """
        å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºJSONæ ¼å¼
        
        å‚æ•°:
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
            json_file_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            è½¬æ¢åçš„æ•°æ®åˆ—è¡¨
        """
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(excel_file_path)
            
            # å®šä¹‰åˆ—åæ˜ å°„
            column_mapping = {
                'ç”¨ä¾‹ID': 'test_case_id',
                'UUID': 'UUID',
                'ç”¨ä¾‹æ ‡é¢˜': 'test_case_title',
                'ç”¨ä¾‹ç›®å½•': 'test_case_menu',
                'æ˜¯å¦è‡ªåŠ¨åŒ–': 'is_automatic',
                'ç­‰çº§': 'level',
                'å‰ç½®æ¡ä»¶': 'prerequisites',
                'æ­¥éª¤æè¿°ç±»å‹': 'step_description_type',
                'æ­¥éª¤æè¿°': 'step_description',
                'é¢„æœŸç»“æœ': 'expected_result'
            }
            
            # è½¬æ¢æ•°æ®
            json_data = []
            for _, row in df.iterrows():
                test_case = {}
                for excel_col, json_key in column_mapping.items():
                    value = row.get(excel_col, "")
                    # å¤„ç†NaNå€¼
                    if pd.isna(value):
                        value = ""
                    else:
                        value = str(value).strip()
                    test_case[json_key] = value
                
                json_data.append(test_case)
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            print(f"æˆåŠŸè½¬æ¢ {len(json_data)} æ¡æµ‹è¯•ç”¨ä¾‹æ•°æ®åˆ° {json_file_path}")
            
            return json_data
            
        except Exception as e:
            raise RuntimeError(f"Excelè½¬JSONå¤±è´¥: {str(e)}")


class TestCaseEvaluator:
    """æµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°å™¨"""
    
    def __init__(self, max_context_tokens: int = 10000):
        self.config = get_config()
        self.api_manager = get_api_manager()
        self.file_manager = get_file_manager()
        self.token_counter = TokenCounter()
        
        # åˆå§‹åŒ–éœ€æ±‚å•çŸ¥è¯†åº“
        self.requirement_kb = RequirementKnowledgeBase()
        
        # åŠ è½½è‡ªå®šä¹‰è§„åˆ™é…ç½®
        self.rules_config = get_test_case_rules()
        print(f"å·²åŠ è½½æµ‹è¯•ç”¨ä¾‹è¯„ä¼°è§„åˆ™é…ç½®: æ ‡é¢˜é•¿åº¦â‰¤{self.rules_config['title_max_length']}å­—ç¬¦, "
              f"æ­¥éª¤æ•°â‰¤{self.rules_config['max_steps']}æ­¥")
        print(f"éœ€æ±‚å•çŸ¥è¯†åº“: å·²åŠ è½½ {len(self.requirement_kb.requirements)} ä¸ªéœ€æ±‚å•")
        
        # ä¸Šä¸‹æ–‡ç®¡ç†ï¼šæ€»context = è¯·æ±‚tokens + å“åº”tokens + æç¤ºè¯æ¨¡æ¿ + ç¼“å†²
        # ä¸ºäº†å®‰å…¨ï¼Œè®¾ç½®è¯·æ±‚tokensä¸ºæ€»ä¸Šä¸‹æ–‡çš„25%ï¼Œå“åº”tokensä¸º50%ï¼Œç•™25%ç¼“å†²
        self.max_context_tokens = max_context_tokens
        
        # æ„å»ºåŠ¨æ€è¯„ä¼°æç¤ºè¯æ¨¡æ¿
        self.evaluation_prompt_template = self._build_dynamic_prompt_template()
        
        # è®¡ç®—æç¤ºè¯æ¨¡æ¿çš„åŸºç¡€tokenæ•°é‡ï¼ˆåŒ…æ‹¬å·²æ›¿æ¢çš„è§„åˆ™å‚æ•°ï¼Œä½†ä¸åŒ…æ‹¬åŠ¨æ€å†…å®¹ï¼‰
        # å…ˆæ›¿æ¢æ‰è§„åˆ™å‚æ•°ï¼ˆè¿™äº›åœ¨å®é™…ä½¿ç”¨æ—¶å·²ç»è¢«æ›¿æ¢ï¼‰ï¼Œä½†ä¿ç•™åŠ¨æ€å ä½ç¬¦
        template_with_rules = self.evaluation_prompt_template
        # åŠ¨æ€å ä½ç¬¦ä¿æŒä¸å˜ï¼Œç”¨äºåç»­æ›¿æ¢
        template_base = template_with_rules.replace('{test_case_id}', '').replace('{test_case_title}', '').replace('{prerequisites}', '').replace('{step_description}', '').replace('{expected_result}', '').replace('{test_cases_json}', '').replace('{requirement_info}', '')
        self.template_base_tokens = self.token_counter.count_tokens(template_base)
        
        # é‡æ–°è®¡ç®—tokené…ç½®ï¼Œå°†æ¨¡æ¿tokensçº³å…¥è€ƒè™‘
        available_tokens = max_context_tokens - self.template_base_tokens  # å‡å»æ¨¡æ¿å ç”¨çš„tokens
        self.max_request_tokens = int(available_tokens * 0.25)  # 25%ç”¨äºè¯·æ±‚ï¼ˆç”¨ä¾‹æ•°æ®éƒ¨åˆ†ï¼‰
        self.max_response_tokens = int(available_tokens * 0.50)  # 50%ç”¨äºå“åº”
        self.token_threshold = int(self.max_request_tokens * 0.75)  # 75%é˜ˆå€¼ï¼Œç¡®ä¿å®‰å…¨

        print(f"Tokené…ç½®: æ€»ä¸Šä¸‹æ–‡={max_context_tokens}, æ¨¡æ¿åŸºç¡€tokens={self.template_base_tokens}, å¯ç”¨tokens={available_tokens}")
        print(f"è¯·æ±‚é™åˆ¶={self.max_request_tokens}, å“åº”é™åˆ¶={self.max_response_tokens}, è¯·æ±‚é˜ˆå€¼={self.token_threshold}")

    def _build_dynamic_prompt_template(self) -> str:
        """
        æ ¹æ®è‡ªå®šä¹‰è§„åˆ™é…ç½®æ„å»ºåŠ¨æ€æç¤ºè¯æ¨¡æ¿
        
        è¿”å›:
            str: åŠ¨æ€ç”Ÿæˆçš„æç¤ºè¯æ¨¡æ¿
        """
        title_max_length = self.rules_config['title_max_length']
        max_steps = self.rules_config['max_steps']
        priority_ratios = self.rules_config['priority_ratios']
        
        # æ„å»ºä¼˜å…ˆçº§å æ¯”è¯´æ˜
        p0_range = f"{priority_ratios['P0']['min']}%~{priority_ratios['P0']['max']}%"
        p1_range = f"{priority_ratios['P1']['min']}%~{priority_ratios['P1']['max']}%"
        p2_range = f"{priority_ratios['P2']['min']}%~{priority_ratios['P2']['max']}%"
        
        template = f"""è¯·ä¸ºä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹åˆ†åˆ«ç”Ÿæˆè¯„ä¼°è¡¨æ ¼ã€‚æ¯ä¸ªç”¨ä¾‹åº”è¯¥æœ‰è‡ªå·±ç‹¬ç«‹çš„è¡¨æ ¼ã€‚

é‡è¦æç¤ºï¼š
1. è¯·ä¸ºæ¯ä¸ªç”¨ä¾‹ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„è¡¨æ ¼
2. æ¯ä¸ªè¡¨æ ¼éƒ½åº”è¯¥åŒ…å«è¡¨å¤´ "| ç”¨ä¾‹ä¿¡æ¯ | åˆ†æ•° | æ”¹è¿›å»ºè®® |"
3. æ¯ä¸ªè¡¨æ ¼ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
4. ä¸¥æ ¼æŒ‰ç…§æä¾›çš„è¡¨æ ¼æ ¼å¼è¾“å‡º
5. **é™¤æ­¤ä»¥å¤–ä¸éœ€è¦ä»»ä½•åˆ†ææˆ–è§£é‡Š**

è¯„åˆ†è§„åˆ™ï¼š

| ç”¨ä¾‹è¦ç´  | æ˜¯å¦å¿…é¡» | è¦æ±‚                                                                                                                         |
| ---- | ---- | -------------------------------------------------------------------------------------------------------------------------- |
| å…³è”éœ€æ±‚ | æ˜¯    | 1. ç”¨ä¾‹åº”å½“ä¸éœ€æ±‚å•ä¸­çš„ä¸€æ¡æˆ–å¤šæ¡æœ‰å…³                                                                                                       |
| ç”¨ä¾‹æ ‡é¢˜ | æ˜¯    | 1. æ ‡é¢˜é•¿åº¦ä¸è¶…è¿‡ {title_max_length} å­—ç¬¦ï¼Œä¸”æè¿°æµ‹è¯•åŠŸèƒ½ç‚¹<br>2. è¯­è¨€æ¸…æ™°ï¼Œç®€æ´æ˜“æ‡‚<br>3. é¿å…åœ¨ç”¨ä¾‹è¯„åˆ†ä¸­å‡ºç°æ­¥éª¤                                               |
| å‰ç½®æ¡ä»¶ | å¦    | 1. åˆ—å‡ºæ‰€æœ‰å‰æï¼šè´¦å·ç±»å‹ï¼Œç°åº¦ç­‰<br>2. é¿å…è¿‡åº¦å¤æ‚ï¼šæ¯ä¸ªæ¡ä»¶ä¸è¶…è¿‡ 2 é¡¹æè¿°ï¼Œå¿…è¦æ—¶åˆ†ç‚¹åˆ—å‡º                                                                      |
| æµ‹è¯•æ­¥éª¤ | æ˜¯    | 1. æ­¥éª¤ç”¨ç¼–å·é“¾æ¥ï¼šä½¿ç”¨ 1ã€2ã€3... ç»“æ„ï¼Œæ¯æ­¥æè¿°ä¸€ä¸ªåŠ¨ä½œ<br>2. æ­¥éª¤å…·ä½“åŒ–ï¼šåŒ…å«ç”¨æˆ·æ“ä½œã€è¾“å…¥å€¼å’Œä¸Šä¸‹æ–‡è¯´æ˜<br>3. æ­¥éª¤æ•°åˆç†ï¼šä¸è¶…è¿‡ {max_steps} æ­¥ï¼Œå¦åˆ™éœ€åˆ†è§£ä¸ºå¤šä¸ªç”¨ä¾‹ã€‚<br>4. é¿å…æ­¥éª¤ä¸­å¸¦æœ‰æ£€æŸ¥ç‚¹ |
| é¢„æœŸç»“æœ | æ˜¯    | 1. æè¿°æ˜ç¡®ç»“æœï¼šæ˜ç¡®çš„ç»“æœï¼Œç¡®åˆ‡çš„æ£€æŸ¥<br>2. é¿å…æ¨¡æ£±ä¸¤å¯çš„è¯ï¼ˆå¦‚åŠŸèƒ½æ­£å¸¸ï¼Œè·Ÿç°ç½‘ä¸€è‡´ç­‰ï¼‰                                                                         |
| ä¼˜å…ˆçº§  | æ˜¯    | 1. ç­‰çº§åˆ’åˆ†æ ‡å‡†ï¼šé‡‡ç”¨ P0-P2<br>2. æ˜ç¡®æ ‡æ³¨ï¼šæ¯ä¸ªç”¨ä¾‹å¿…é¡»æœ‰ä¼˜å…ˆçº§å­—æ®µã€‚<br>3. ä¼˜å…ˆçº§æ¯”ä¾‹ï¼šP0å æ¯”åº”åœ¨ {p0_range} ä¹‹é—´ï¼ŒP1å æ¯” {p0_range}ï¼ŒP2å æ¯” {p0_range}               |

éœ€æ±‚å•ä¿¡æ¯ï¼š
{{requirement_info}}

* æ‰€æœ‰è¯„åˆ†æ»¡åˆ†ä¸º 10 åˆ†ï¼Œæœªæä¾›å¿…é¡»æä¾›çš„å­—æ®µç»™0åˆ†ï¼Œæ¯æœ‰ä¸€ç‚¹è¦æ±‚æœªæ»¡è¶³åˆ™é…Œæƒ…æ‰£1-2åˆ†ï¼Œæœ€ä½ 0 åˆ†ã€‚
* è‹¥ç”¨ä¾‹ä¸éœ€æ±‚å•ä¸­ä»»ä½•ä¸€æ¡éœ€æ±‚éƒ½æ— å…³ï¼Œåˆ™ç»™0åˆ†ã€‚è‹¥ä¸æŸä¸€æ¡éœ€æ±‚ç›¸å…³ç¨‹åº¦è¾ƒé«˜ï¼Œåˆ™æ ¹æ®ç›¸å…³ç¨‹åº¦ç»™å‡ºåˆ†æ•°ã€‚è‹¥åŒæ—¶ä¸å¤šæ¡éœ€æ±‚ç›¸å…³ï¼Œåˆ™æ ¹æ®ç›¸å…³ç¨‹åº¦ç»¼åˆè¯„åˆ†ã€‚
* å¯¹ä½äº 10 åˆ†çš„è¦ç´ ç»™å‡ºå‡†ç¡®çš„å»ºè®®ã€‚ç»™å‡ºçš„æ¯ä¸€æ¡å»ºè®®éƒ½åº”å½“ä¸€é’ˆè§è¡€ï¼Œå­—æ•°åœ¨10-30ä¹‹é—´ã€‚

å¯¹äºæ¯ä¸€æ¡ç”¨ä¾‹ï¼Œåœ¨ä½ çš„å›ç­”ä¸­è¯„åˆ†ä¸å»ºè®®çš„æ ¼å¼å¦‚ä¸‹ï¼ˆä½ çš„å›ç­”åªéœ€æä¾›æ­¤ç±»è¡¨æ ¼ï¼Œåº”ä¸¥æ ¼æŒ‰ç…§æ­¤æ ‡å‡†æ‰§è¡Œï¼‰ï¼š

| ç”¨ä¾‹ä¿¡æ¯                           | åˆ†æ•°  | æ”¹è¿›å»ºè®®                |
| ------------------------------ | --- | ------------------- |
| **ç”¨ä¾‹ID**<br>{{test_case_id}}     | -   | -                   |
| **ç”¨ä¾‹æ ‡é¢˜**<br>{{test_case_title}}  | 8   | æ”¹ä¸º"éªŒè¯é”™è¯¯å¯†ç ç™»å½•çš„å¤±è´¥æç¤º"   |
| **å‰ç½®æ¡ä»¶**<br>{{prerequisites}}    | 8   | è¡¥å……ç³»ç»Ÿç‰ˆæœ¬è¦æ±‚            |
| **æµ‹è¯•æ­¥éª¤**<br>{{step_description}} | 6   | æ­¥éª¤3å¢åŠ "ç­‰å¾…3ç§’"         |
| **é¢„æœŸç»“æœ**<br>{{expected_result}}  | 7   | æ˜ç¡®æç¤ºä½ç½®ï¼ˆå¦‚ï¼šè¾“å…¥æ¡†ä¸‹æ–¹çº¢è‰²æ–‡å­—ï¼‰ |

æµ‹è¯•ç”¨ä¾‹ï¼š
{{test_cases_json}}
"""
        return template

    def estimate_batch_tokens(self, test_cases: List[Dict[str, Any]]) -> int:
        """
        ä¼°ç®—ä¸€æ‰¹æµ‹è¯•ç”¨ä¾‹åœ¨æ‰¹é‡å¤„ç†æ—¶éœ€è¦çš„tokenæ•°é‡
        
        å‚æ•°:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            
        è¿”å›:
            é¢„è®¡tokenæ•°é‡
        """
        if not test_cases:
            return 0
            
        # è·å–éœ€æ±‚å•ä¿¡æ¯
        requirement_info = self.requirement_kb.get_requirements_for_evaluation()
        
        # æ„å»ºä¸evaluate_batchä¸€è‡´çš„æ‰¹é‡æç¤ºè¯
        test_cases_json = json.dumps(test_cases, ensure_ascii=False, indent=2)
        
        prompt = self.evaluation_prompt_template.format(
            test_case_id="[æ‰¹é‡å¤„ç†]",
            test_case_title="[æ‰¹é‡å¤„ç†]",
            prerequisites="[æ‰¹é‡å¤„ç†]",
            step_description="[æ‰¹é‡å¤„ç†]",
            expected_result="[æ‰¹é‡å¤„ç†]",
            requirement_info=requirement_info,
            test_cases_json=test_cases_json
        )
        
        # è®¡ç®—tokenæ•°é‡
        return self.token_counter.count_tokens(prompt)
    
    def split_test_cases_by_tokens(self, test_cases: List[Dict[str, Any]], 
                                 start_index: int = 0) -> Tuple[List[Dict[str, Any]], int]:
        """
        æ ¹æ®tokené™åˆ¶åˆ†å‰²æµ‹è¯•ç”¨ä¾‹
        
        å‚æ•°:
            test_cases: å…¨éƒ¨æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            start_index: å¼€å§‹ç´¢å¼•
            
        è¿”å›:
            (å½“å‰æ‰¹æ¬¡çš„æµ‹è¯•ç”¨ä¾‹, ä¸‹ä¸€æ‰¹æ¬¡çš„å¼€å§‹ç´¢å¼•)
        """
        current_batch = []
        current_tokens = 0
        
        for i in range(start_index, len(test_cases)):
            # å°è¯•æ·»åŠ ä¸‹ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹
            candidate_batch = current_batch + [test_cases[i]]
            candidate_tokens = self.estimate_batch_tokens(candidate_batch)
            
            if candidate_tokens > self.token_threshold and current_batch:
                # è¶…è¿‡é˜ˆå€¼ä¸”å½“å‰æ‰¹æ¬¡ä¸ä¸ºç©ºï¼Œåœæ­¢æ·»åŠ 
                break
            
            # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
            current_batch.append(test_cases[i])
            current_tokens = candidate_tokens
            
        next_index = start_index + len(current_batch)
        
        # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡åŒ…å«çš„æµ‹è¯•ç”¨ä¾‹ID
        batch_ids = [case.get('test_case_id', 'N/A') for case in current_batch]
        print(f"å½“å‰æ‰¹æ¬¡åŒ…å« {len(current_batch)} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œé¢„è®¡ä½¿ç”¨ {current_tokens} tokens")
        print(f"æ‰¹æ¬¡åŒ…å«çš„ç”¨ä¾‹ID: {', '.join(batch_ids)}")
        
        return current_batch, next_index
    
    async def evaluate_batch(self, test_cases: List[Dict[str, Any]], 
                           session: aiohttp.ClientSession) -> str:
        """
        è¯„ä¼°ä¸€æ‰¹æµ‹è¯•ç”¨ä¾‹
        
        å‚æ•°:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            session: HTTPä¼šè¯
            
        è¿”å›:
            AIè¯„ä¼°ç»“æœ
        """
        # æ„å»ºæ‰¹é‡æç¤ºè¯ - ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
        test_cases_json = json.dumps(test_cases, ensure_ascii=False, indent=2)
        
        # è·å–éœ€æ±‚å•ä¿¡æ¯
        requirement_info = self.requirement_kb.get_requirements_for_evaluation()
        
        # ç›´æ¥ä½¿ç”¨æ¨¡æ¿æ„å»ºæœ€ç»ˆæç¤ºè¯ï¼ˆæ¨¡æ¿å·²åŒ…å«æ‰€æœ‰å¿…è¦æŒ‡ä»¤ï¼‰
        final_prompt = self.evaluation_prompt_template.format(
            test_case_id="[æ‰¹é‡å¤„ç†]",
            test_case_title="[æ‰¹é‡å¤„ç†]",
            prerequisites="[æ‰¹é‡å¤„ç†]",
            step_description="[æ‰¹é‡å¤„ç†]",
            expected_result="[æ‰¹é‡å¤„ç†]",
            requirement_info=requirement_info,
            test_cases_json=test_cases_json
        )
        
        # è®¡ç®—è¯·æ±‚tokenæ•°é‡ï¼ŒåŠ¨æ€è®¾ç½®å“åº”tokené™åˆ¶
        request_tokens = self.token_counter.count_tokens(final_prompt)
        # å“åº”tokenåº”è¯¥ä¸è¯·æ±‚tokenå¤§è‡´ç›¸ç­‰ï¼Œä½†ä¸è¶…è¿‡æœ€å¤§å“åº”é™åˆ¶
        dynamic_response_tokens = min(request_tokens * 2, self.max_response_tokens)  # æ‰¹é‡å¤„ç†éœ€è¦æ›´å¤šå“åº”ç©ºé—´
        
        print(f"æ‰¹é‡å¤„ç† {len(test_cases)} ä¸ªç”¨ä¾‹: è¯·æ±‚tokens={request_tokens}, å“åº”tokensé™åˆ¶={dynamic_response_tokens}")
        
        # æ˜¾ç¤ºæ­£åœ¨å¤„ç†çš„æµ‹è¯•ç”¨ä¾‹ID
        processing_ids = [case.get('test_case_id', 'N/A') for case in test_cases]
        print(f"ğŸ”„ æ­£åœ¨å¤„ç†çš„æµ‹è¯•ç”¨ä¾‹: {', '.join(processing_ids)}")
        print("ğŸ“¤ æ­£åœ¨è°ƒç”¨AIè¿›è¡Œè¯„ä¼°...")
        
        # è°ƒç”¨AI API
        result = await self.api_manager.call_llm(
            prompt=final_prompt,
            session=session,
            max_tokens=dynamic_response_tokens
        )
        
        print("âœ… AIè¯„ä¼°å®Œæˆï¼Œå¼€å§‹è§£æç»“æœ...")
        return result
    
    def parse_evaluation_result(self, ai_response: str) -> List[Dict[str, Any]]:
        """
        è§£æAIè¯„ä¼°ç»“æœï¼Œå°†Markdownè¡¨æ ¼è½¬æ¢ä¸ºJSON
        æ”¯æŒè§£æå¤šä¸ªè¡¨æ ¼ï¼ˆå½“AIè¿”å›å¤šä¸ªè¡¨æ ¼æ—¶ï¼‰
        
        å‚æ•°:
            ai_response: AIè¿”å›çš„Markdownè¡¨æ ¼
            
        è¿”å›:
            è§£æåçš„è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        evaluations = []
        
        # æ¸…ç†å“åº”ï¼Œæ‰¾åˆ°æ‰€æœ‰è¡¨æ ¼éƒ¨åˆ†
        lines = ai_response.split('\n')
        all_tables = []
        current_table_lines = []
        table_started = False
        
        for line in lines:
            line = line.strip()
            
            # æ£€æµ‹è¡¨æ ¼å¼€å§‹
            if '| ç”¨ä¾‹ä¿¡æ¯ |' in line or '| --- |' in line:
                # å¦‚æœå·²ç»åœ¨å¤„ç†è¡¨æ ¼ï¼Œå…ˆä¿å­˜å½“å‰è¡¨æ ¼
                if table_started and current_table_lines:
                    all_tables.append(current_table_lines[:])
                    current_table_lines = []
                table_started = True
                continue
            
            # è¡¨æ ¼å†…å®¹è¡Œ
            elif table_started and line.startswith('|') and len(line.split('|')) >= 4:
                current_table_lines.append(line)
            
            # è¡¨æ ¼ç»“æŸï¼ˆç©ºè¡Œæˆ–éè¡¨æ ¼è¡Œï¼‰
            elif table_started and (not line or not line.startswith('|')):
                if current_table_lines:
                    all_tables.append(current_table_lines[:])
                    current_table_lines = []
                table_started = False
        
        # å¤„ç†æœ€åä¸€ä¸ªè¡¨æ ¼
        if table_started and current_table_lines:
            all_tables.append(current_table_lines)
        
        print(f"æ‰¾åˆ° {len(all_tables)} ä¸ªè¡¨æ ¼")
        print("ğŸ” å¼€å§‹è§£æè¡¨æ ¼æ•°æ®...")
        
        if not all_tables:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¡¨æ ¼æ•°æ®")
            return evaluations
        
        # è§£ææ¯ä¸ªè¡¨æ ¼
        for table_index, table_lines in enumerate(all_tables):
            print(f"ğŸ“‹ è§£æç¬¬ {table_index + 1} ä¸ªè¡¨æ ¼ï¼ŒåŒ…å« {len(table_lines)} è¡Œ")
            
            current_case = None
            case_id = None
            
            for line in table_lines:
                parts = [part.strip() for part in line.split('|')[1:-1]]
                if len(parts) >= 3:
                    field_info = parts[0]
                    score = parts[1]
                    suggestion = parts[2]
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç”¨ä¾‹ID
                    if '**ç”¨ä¾‹ID**' in field_info:
                        # æå–ç”¨ä¾‹ID
                        if '<br>' in field_info:
                            id_part = field_info.split('<br>')[-1].strip()
                        else:
                            # å¤„ç†æ²¡æœ‰<br>çš„æƒ…å†µï¼Œç›´æ¥ä»field_infoä¸­æå–
                            id_part = field_info.replace('**ç”¨ä¾‹ID**', '').strip()
                        
                        case_id = id_part
                        print(f"  ğŸ“ æ­£åœ¨è§£æç”¨ä¾‹ID: {case_id}")
                        
                        # ä¿å­˜ä¹‹å‰çš„ç”¨ä¾‹
                        if current_case:
                            evaluations.append(current_case)
                        
                        # å¼€å§‹æ–°ç”¨ä¾‹
                        current_case = {
                            'test_case_id': case_id,
                            'evaluations': []
                        }
                        
                    elif current_case and '**' in field_info:
                        # æå–å­—æ®µåå’Œå†…å®¹ï¼Œæ­£ç¡®å¤„ç†å¤šè¡Œå†…å®¹ï¼ˆ<br>åˆ†éš”ï¼‰
                        if '<br>' in field_info:
                            field_parts = field_info.split('<br>')
                            field_name = field_parts[0].replace('**', '').strip()
                            # åˆå¹¶æ‰€æœ‰å†…å®¹éƒ¨åˆ†ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
                            field_content = '<br>'.join(field_parts[1:]).strip() if len(field_parts) > 1 else ''
                            # å°†<br>è½¬æ¢ä¸ºå®é™…æ¢è¡Œç¬¦ï¼Œä¾¿äºé˜…è¯»
                            field_content = field_content.replace('<br>', '\n')
                        else:
                            # å¤„ç†æ²¡æœ‰<br>çš„æƒ…å†µ
                            field_name = field_info.replace('**', '').strip()
                            field_content = ''
                        
                        print(f"    ğŸ“Š è§£æå­—æ®µ: {field_name} (åˆ†æ•°: {score.strip() if score.strip() != '-' else 'æ— '})")
                        
                        evaluation_item = {
                            'field': field_name,
                            'content': field_content,
                            'score': score.strip() if score.strip() != '-' else None,
                            'suggestion': suggestion.strip() if suggestion.strip() != '-' else None
                        }
                        
                        current_case['evaluations'].append(evaluation_item)
            
            # æ·»åŠ å½“å‰è¡¨æ ¼çš„æœ€åä¸€ä¸ªç”¨ä¾‹
            if current_case:
                evaluations.append(current_case)
                print(f"  âœ… å®Œæˆç”¨ä¾‹è§£æ: {current_case['test_case_id']}")
        
        print(f"ğŸ‰ æˆåŠŸè§£æ {len(evaluations)} ä¸ªç”¨ä¾‹çš„è¯„ä¼°ç»“æœ")
        if evaluations:
            # è¾“å‡ºè§£æç»“æœçš„ç¤ºä¾‹
            first_case = evaluations[0]
            print(f"ğŸ“Š ç¤ºä¾‹è§£æç»“æœ - ç”¨ä¾‹ID: {first_case['test_case_id']}, è¯„ä¼°é¡¹æ•°: {len(first_case['evaluations'])}")
            for item in first_case['evaluations'][:2]:  # æ˜¾ç¤ºå‰ä¸¤ä¸ªè¯„ä¼°é¡¹
                print(f"    - {item['field']}: åˆ†æ•°={item['score']}, å»ºè®®={item['suggestion']}")
        
        return evaluations
    
    async def evaluate_test_cases(self, test_cases: List[Dict[str, Any]], 
                                test_batch_count: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è¯„ä¼°æµ‹è¯•ç”¨ä¾‹
        
        å‚æ•°:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            test_batch_count: æµ‹è¯•æ•°æ®æ‰¹æ¬¡ï¼Œ1è¡¨ç¤ºåªå¤„ç†ç¬¬ä¸€æ‰¹
            
        è¿”å›:
            è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        all_evaluations = []
        current_index = 0
        batch_number = 1
        
        async with aiohttp.ClientSession() as session:
            while current_index < len(test_cases):
                # å¦‚æœè®¾ç½®äº†æµ‹è¯•æ‰¹æ¬¡é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦è¶…è¿‡
                if test_batch_count and batch_number > test_batch_count:
                    print(f"ğŸ›‘ è¾¾åˆ°æµ‹è¯•æ‰¹æ¬¡é™åˆ¶ ({test_batch_count})ï¼Œåœæ­¢å¤„ç†")
                    break
                
                # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
                remaining_cases = len(test_cases) - current_index
                progress_percent = (current_index / len(test_cases)) * 100
                print(f"\nğŸ“Š æ€»ä½“è¿›åº¦: {current_index}/{len(test_cases)} ({progress_percent:.1f}%), å‰©ä½™ {remaining_cases} ä¸ªç”¨ä¾‹")
                print(f"ğŸš€ å¼€å§‹å¤„ç†ç¬¬ {batch_number} æ‰¹æ¬¡...")
                
                # åˆ†å‰²å½“å‰æ‰¹æ¬¡
                batch_cases, next_index = self.split_test_cases_by_tokens(
                    test_cases, current_index
                )
                
                if not batch_cases:
                    print("âœ… æ²¡æœ‰æ›´å¤šæµ‹è¯•ç”¨ä¾‹å¯å¤„ç†")
                    break
                
                try:
                    # è¯„ä¼°å½“å‰æ‰¹æ¬¡
                    ai_result = await self.evaluate_batch(batch_cases, session)
                    print(f"ğŸ“„ AIè¿”å›ç»“æœé•¿åº¦: {len(ai_result)}")
                    print(f"ğŸ” AIè¿”å›ç»“æœå‰1000å­—ç¬¦é¢„è§ˆ: ========================================")
                    print(f"\n{ai_result[:1000]}\n......")
                    print("============================================================")
                    
                    # è§£æç»“æœ
                    batch_evaluations = self.parse_evaluation_result(ai_result)
                    all_evaluations.extend(batch_evaluations)
                    
                    # æ˜¾ç¤ºæœ¬æ‰¹æ¬¡å¤„ç†çš„ç”¨ä¾‹ID
                    processed_ids = [eval_result['test_case_id'] for eval_result in batch_evaluations]
                    print(f"âœ… ç¬¬ {batch_number} æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œè¯„ä¼°äº† {len(batch_evaluations)} ä¸ªç”¨ä¾‹")
                    print(f"ğŸ“‹ å·²å®Œæˆè¯„ä¼°çš„ç”¨ä¾‹ID: {', '.join(processed_ids)}")
                    
                    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ä¸”ç¬¬ä¸€æ‰¹æ¬¡å®Œæˆï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
                    if test_batch_count == 1 and batch_number == 1:
                        print(f"\nğŸ§ª ç¬¬ä¸€æ‰¹æ¬¡æµ‹è¯•å®Œæˆï¼Œè¯„ä¼°ç»“æœé¢„è§ˆ:")
                        if batch_evaluations:
                            first_eval = batch_evaluations[0]
                            print(f"ğŸ“Š ç”¨ä¾‹ID: {first_eval['test_case_id']}")
                            print(f"ğŸ“ˆ è¯„ä¼°é¡¹æ•°é‡: {len(first_eval['evaluations'])}")
                        else:
                            print("âŒ è§£æè¯„ä¼°ç»“æœå¤±è´¥ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è§£æé€»è¾‘")
                            print(f"ğŸ” AIåŸå§‹è¿”å›: {ai_result[:1000]}...")
                        print("\nğŸ’¡ å¦‚éœ€å¤„ç†æ›´å¤šæ‰¹æ¬¡ï¼Œè¯·ä¿®æ”¹ test_batch_count å‚æ•°")
                        break
                    
                except Exception as e:
                    print(f"âŒ ç¬¬ {batch_number} æ‰¹æ¬¡å¤„ç†å¤±è´¥: {str(e)}")
                    if test_batch_count == 1:
                        print("ğŸ”§ æµ‹è¯•æ‰¹æ¬¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®å’Œç½‘ç»œè¿æ¥")
                        break
                    else:
                        print("â­ï¸ è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡")
                
                current_index = next_index
                batch_number += 1
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                await asyncio.sleep(1)
        
        return all_evaluations


async def main_process(test_batch_count: Optional[int] = None):
    """
    ä¸»å¤„ç†æµç¨‹
    
    å‚æ•°:
        test_batch_count: æµ‹è¯•æ•°æ®æ‰¹æ¬¡é™åˆ¶ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ•°æ®
    """
    from datetime import datetime
    from collections import Counter
    start_time = datetime.now()
    start_time_str = start_time.strftime("%Y-%m-%d %H:%M:%S")
    print(f"\nâ±ï¸ å¼€å§‹å¤„ç†æ—¶é—´: {start_time_str}")
    
    config = get_config()
    processor = TestCaseProcessor()
    # 12Kæ€»ä¸Šä¸‹æ–‡ï¼š3.6Kè¯·æ±‚ + 7.2Kå“åº” + 2.4Kç¼“å†²
    evaluator = TestCaseEvaluator(max_context_tokens=12000)
    
    # è·å–è§„åˆ™é…ç½®ä¸­çš„ä¼˜å…ˆçº§æ¯”ä¾‹è¦æ±‚
    rules_config = get_test_case_rules()
    priority_ratios = rules_config['priority_ratios']
    
    # æ–‡ä»¶è·¯å¾„
    excel_file = config.local_data_path / "TestCase_20250717141033-32202633.xlsx"
    json_file = config.local_data_path / "TestCase_20250717141033-32202633.json"
    result_file = config.local_data_path / "Proceed_TestCase_20250717141033-32202633.json"
    
    try:
        # æ­¥éª¤1: æ£€æŸ¥Excelæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not excel_file.exists():
            raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_file}")
        
        # æ­¥éª¤2: å°†Excelè½¬æ¢ä¸ºJSONï¼ˆå¦‚æœJSONæ–‡ä»¶ä¸å­˜åœ¨ï¼‰
        if not json_file.exists():
            print("å¼€å§‹è½¬æ¢Excelæ–‡ä»¶ä¸ºJSONæ ¼å¼...")
            test_cases = processor.excel_to_json(str(excel_file), str(json_file))
        else:
            print("JSONæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½...")
            with open(json_file, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
        
        print(f"åŠ è½½äº† {len(test_cases)} æ¡æµ‹è¯•ç”¨ä¾‹æ•°æ®")
        
        # åˆ†ææµ‹è¯•ç”¨ä¾‹çš„ä¼˜å…ˆçº§åˆ†å¸ƒ
        level_counter = Counter()
        for test_case in test_cases:
            level = test_case.get('level', '').strip().upper()
            if level:
                # æ ‡å‡†åŒ–ä¼˜å…ˆçº§è¡¨ç¤ºï¼Œä¾‹å¦‚P0ã€P1ã€P2
                if level.startswith('P') and len(level) > 1 and level[1].isdigit():
                    level_counter[level] += 1
                else:
                    level_counter['å…¶ä»–'] += 1
            else:
                level_counter['æœªè®¾ç½®'] += 1
        
        # è®¡ç®—å„ä¼˜å…ˆçº§å æ¯”
        total_cases = len(test_cases)
        level_percentages = {}
        level_compliance = {}
        
        print("\nğŸ“Š æµ‹è¯•ç”¨ä¾‹ä¼˜å…ˆçº§åˆ†å¸ƒåˆ†æï¼š")
        for level, count in level_counter.items():
            percentage = (count / total_cases) * 100
            level_percentages[level] = percentage
            
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆè§„åˆ™é…ç½®
            is_compliant = False
            reason = "æœªæ‰¾åˆ°å¯¹åº”è§„åˆ™"
            
            if level in ['P0', 'P1', 'P2']:
                level_key = level  # ä½¿ç”¨åŸå§‹çš„P0ã€P1ã€P2ä½œä¸ºé”®
                if level_key in priority_ratios:
                    min_percent = priority_ratios[level_key]['min']
                    max_percent = priority_ratios[level_key]['max']
                    is_compliant = min_percent <= percentage <= max_percent
                    if is_compliant:
                        reason = f"ç¬¦åˆè¦æ±‚ï¼š{min_percent}% ~ {max_percent}%"
                    else:
                        if percentage < min_percent:
                            reason = f"ä½äºæœ€å°è¦æ±‚ï¼š{percentage:.1f}% < {min_percent}%"
                        else:
                            reason = f"è¶…è¿‡æœ€å¤§è¦æ±‚ï¼š{percentage:.1f}% > {max_percent}%"
            
            level_compliance[level] = {
                "count": count,
                "percentage": percentage,
                "is_compliant": is_compliant,
                "reason": reason
            }
            
            compliance_icon = "âœ…" if is_compliant else "âŒ"
            print(f"{compliance_icon} {level}: {count} æ¡ ({percentage:.1f}%) - {reason}")
        
        # æ­¥éª¤3: AIè¯„ä¼°
        batch_limit_text = f"ï¼Œæµ‹è¯•æ‰¹æ¬¡é™åˆ¶: {test_batch_count}" if test_batch_count else "ï¼Œå°†å¤„ç†æ‰€æœ‰æ•°æ®"
        print(f"\nğŸš€ å¼€å§‹AIè¯„ä¼°{batch_limit_text}")
        print(f"ğŸ“Š æ€»è®¡éœ€è¦è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        evaluations = await evaluator.evaluate_test_cases(test_cases, test_batch_count)
        
        # æ­¥éª¤4: ä¿å­˜è¯„ä¼°ç»“æœ
        if evaluations:
            file_manager = get_file_manager()
            # åŒ…è£…ä¸ºå­—å…¸æ ¼å¼ä»¥ç¬¦åˆsave_json_dataçš„æ¥å£ï¼Œå¹¶æ·»åŠ ä¼˜å…ˆçº§åˆ†æç»“æœ
            result_data = {
                "evaluation_results": evaluations,
                "total_count": len(evaluations),
                "generated_at": str(json_file).split('_')[-1].replace('.json', ''),
                "process_start_time": start_time_str,
                "process_end_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "priority_analysis": {
                    "distribution": level_percentages,
                    "compliance": level_compliance,
                    "rules": priority_ratios
                }
            }
            file_manager.save_json_data(result_data, str(result_file))
            print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            print(f"ğŸ“ˆ å…±è¯„ä¼°äº† {len(evaluations)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        else:
            print("\nâŒ æ²¡æœ‰ç”Ÿæˆè¯„ä¼°ç»“æœ")
            
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {str(e)}")
        raise
    
    # è®¡ç®—å¤„ç†æ—¶é—´
    end_time = datetime.now()
    end_time_str = end_time.strftime("%Y-%m-%d %H:%M:%S")
    total_seconds = (end_time - start_time).total_seconds()
    
    print(f"\nâ±ï¸ ç»“æŸå¤„ç†æ—¶é—´: {end_time_str}")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_seconds:.2f} ç§’")
    
    # è®¡ç®—å¹³å‡æ¯æ¡æ•°æ®çš„å¤„ç†æ—¶é—´
    if evaluations and len(evaluations) > 0:
        avg_time_per_case = total_seconds / len(evaluations)
        print(f"â±ï¸ å¹³å‡æ¯æ¡æ•°æ®å¤„ç†æ—¶é—´: {avg_time_per_case:.2f} ç§’/æ¡")
    else:
        print("â±ï¸ æ— æ³•è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´ï¼šæ²¡æœ‰æˆåŠŸå¤„ç†çš„æ•°æ®")


if __name__ == "__main__":
    print(r" ______     __        ______     __   __   ______     __         __  __     ______     ______   ______     ______    ")
    print(r"/\  __ \   /\ \      /\  ___\   /\ \ / /  /\  __ \   /\ \       /\ \/\ \   /\  __ \   /\__  _\ /\  __ \   /\  == \   ")
    print(r"\ \  __ \  \ \ \     \ \  __\   \ \ \'/   \ \  __ \  \ \ \____  \ \ \_\ \  \ \  __ \  \/_/\ \/ \ \ \/\ \  \ \  __<   ")
    print(r" \ \_\ \_\  \ \_\     \ \_____\  \ \__|    \ \_\ \_\  \ \_____\  \ \_____\  \ \_\ \_\    \ \_\  \ \_____\  \ \_\ \_\ ")
    print(r"  \/_/\/_/   \/_/      \/_____/   \/_/      \/_/\/_/   \/_____/   \/_____/   \/_/\/_/     \/_/   \/_____/   \/_/ /_/ ")

    # æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†3æ‰¹æ•°æ®
    # asyncio.run(main_process(test_batch_count=3))

    # æ­£å¼æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ•°æ®
    asyncio.run(main_process())  # ä¸ä¼ å…¥test_batch_countå‚æ•°ï¼Œå°†å¤„ç†æ‰€æœ‰æ•°æ®
