"""
AIè¯„ä¼°ç”¨ä¾‹ - æ‰“åˆ†ä¸å»ºè®®åŠŸèƒ½

ä»Excelæ–‡ä»¶è¯»å–æµ‹è¯•ç”¨ä¾‹æ•°æ®ï¼Œé€šè¿‡AIè¿›è¡Œè¯„ä¼°æ‰“åˆ†å¹¶ç»™å‡ºæ”¹è¿›å»ºè®®
æ”¯æŒæ‰¹æ¬¡å¤„ç†ä»¥æ§åˆ¶tokenä½¿ç”¨é‡
"""

import os
import re
import json
import asyncio
import aiohttp
import pandas as pd
import sys
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œmcp_toolsç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
mcp_tools_path = project_root / "mcp_tools"

if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(mcp_tools_path) not in sys.path:
    sys.path.insert(0, str(mcp_tools_path))

# å¯¼å…¥å…¬å…±å·¥å…·
from common_utils import get_config, get_api_manager, get_file_manager
from test_case_rules_customer import get_test_case_rules


class TokenCounter:
    """Tokenè®¡æ•°å™¨ - æ”¯æŒtransformers tokenizerå’Œæ”¹è¿›çš„é¢„ä¼°æ¨¡å¼"""
    
    def __init__(self):
        self.config = get_config()
        self.tokenizer = None
        self._try_load_tokenizer()
    
    def _try_load_tokenizer(self):
        """å°è¯•åŠ è½½DeepSeek tokenizer"""
        try:
            # å°è¯•ä½¿ç”¨transformersåº“åŠ è½½tokenizer
            import transformers
            
            tokenizer_path = self.config.models_path / "deepseek_v3_tokenizer" / "deepseek_v3_tokenizer"
            
            if tokenizer_path.exists():
                self.tokenizer = transformers.AutoTokenizer.from_pretrained(
                    str(tokenizer_path), 
                    trust_remote_code=True
                )
                print("å·²åŠ è½½DeepSeek tokenizerï¼ˆtransformersï¼‰ï¼Œå°†ä½¿ç”¨ç²¾ç¡®tokenè®¡æ•°")
                return
            else:
                print(f"DeepSeek tokenizerè·¯å¾„ä¸å­˜åœ¨: {tokenizer_path}")
                
        except ImportError as e:
            print(f"transformersåº“æœªå®‰è£…: {e}")
        except Exception as e:
            print(f"åŠ è½½DeepSeek tokenizerå¤±è´¥: {e}")
        
        print("å°†ä½¿ç”¨æ”¹è¿›çš„é¢„ä¼°æ¨¡å¼è¿›è¡Œtokenè®¡æ•°")
        self.tokenizer = None
    
    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        å‚æ•°:
            text: è¦è®¡ç®—çš„æ–‡æœ¬
            
        è¿”å›:
            tokenæ•°é‡
        """
        if self.tokenizer:
            try:
                tokens = self.tokenizer.encode(text)
                return len(tokens)
            except Exception as e:
                print(f"ä½¿ç”¨tokenizerè®¡æ•°å¤±è´¥: {e}ï¼Œè½¬ä¸ºé¢„ä¼°æ¨¡å¼")
        
        # æ”¹è¿›çš„é¢„ä¼°æ¨¡å¼ï¼šåŸºäºæµ‹è¯•ç»“æœä¼˜åŒ–å‚æ•°
        # æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼š
        # - æ ·æœ¬æ–‡æœ¬å¹³å‡æ¯”ç‡: 0.98 (é¢„ä¼°vså®é™…)
        # - çœŸå®ç”¨ä¾‹å¹³å‡æ¯”ç‡: 0.91 (é¢„ä¼°vså®é™…)
        # - é¢„ä¼°æ¨¡å¼æ€»ä½“åä½çº¦10%ï¼Œéœ€è¦è°ƒæ•´ç³»æ•°
        
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        digits = len(re.findall(r'[0-9]', text))
        punctuation = len(re.findall(r'[^\w\s]', text))
        spaces = len(re.findall(r'\s', text))
        other_chars = len(text) - chinese_chars - english_chars - digits - punctuation - spaces
        
        # åŸºäºæµ‹è¯•ç»“æœè°ƒæ•´çš„ç³»æ•°
        estimated_tokens = int(
            chinese_chars * 0.7 +      # ä¸­æ–‡å­—ç¬¦ï¼ˆä»0.6è°ƒæ•´åˆ°0.7ï¼‰
            english_chars * 0.35 +     # è‹±æ–‡å­—ç¬¦ï¼ˆä»0.3è°ƒæ•´åˆ°0.35ï¼‰
            digits * 0.3 +             # æ•°å­—
            punctuation * 0.4 +        # æ ‡ç‚¹ç¬¦å·
            spaces * 0.1 +             # ç©ºæ ¼
            other_chars * 0.4          # å…¶ä»–å­—ç¬¦
        )
        
        # æ·»åŠ 10%çš„ä¿®æ­£ç³»æ•°ï¼Œå› ä¸ºæµ‹è¯•æ˜¾ç¤ºé¢„ä¼°åä½
        estimated_tokens = int(estimated_tokens * 1.1)
        
        return estimated_tokens


class TestCaseProcessor:
    """Excelæµ‹è¯•ç”¨ä¾‹å¤„ç†å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.file_manager = get_file_manager()
        
    def excel_to_json(self, excel_file_path: str, json_file_path: str) -> List[Dict[str, Any]]:
        """
        å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºJSONæ ¼å¼
        
        å‚æ•°:
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
            json_file_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            è½¬æ¢åçš„æ•°æ®åˆ—è¡¨
        """
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(excel_file_path)
            
            # å®šä¹‰åˆ—åæ˜ å°„
            column_mapping = {
                'ç”¨ä¾‹ID': 'test_case_id',
                'UUID': 'UUID',
                'ç”¨ä¾‹æ ‡é¢˜': 'test_case_title',
                'ç”¨ä¾‹ç›®å½•': 'test_case_menu',
                'æ˜¯å¦è‡ªåŠ¨åŒ–': 'is_automatic',
                'ç­‰çº§': 'level',
                'å‰ç½®æ¡ä»¶': 'prerequisites',
                'æ­¥éª¤æè¿°ç±»å‹': 'step_description_type',
                'æ­¥éª¤æè¿°': 'step_description',
                'é¢„æœŸç»“æœ': 'expected_result'
            }
            
            # è½¬æ¢æ•°æ®
            json_data = []
            for _, row in df.iterrows():
                test_case = {}
                for excel_col, json_key in column_mapping.items():
                    value = row.get(excel_col, "")
                    # å¤„ç†NaNå€¼
                    if pd.isna(value):
                        value = ""
                    else:
                        value = str(value).strip()
                    test_case[json_key] = value
                
                json_data.append(test_case)
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            print(f"æˆåŠŸè½¬æ¢ {len(json_data)} æ¡æµ‹è¯•ç”¨ä¾‹æ•°æ®åˆ° {json_file_path}")
            
            return json_data
            
        except Exception as e:
            raise RuntimeError(f"Excelè½¬JSONå¤±è´¥: {str(e)}")


class TestCaseEvaluator:
    """æµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°å™¨"""
    
    def __init__(self, max_context_tokens: int = 10000):
        self.config = get_config()
        self.api_manager = get_api_manager()
        self.file_manager = get_file_manager()
        self.token_counter = TokenCounter()
        
        # åŠ è½½è‡ªå®šä¹‰è§„åˆ™é…ç½®
        self.rules_config = get_test_case_rules()
        print(f"å·²åŠ è½½æµ‹è¯•ç”¨ä¾‹è¯„ä¼°è§„åˆ™é…ç½®: æ ‡é¢˜é•¿åº¦â‰¤{self.rules_config['title_max_length']}å­—ç¬¦, "
              f"æ­¥éª¤æ•°â‰¤{self.rules_config['max_steps']}æ­¥")
        
        # ä¸Šä¸‹æ–‡ç®¡ç†ï¼šæ€»context = è¯·æ±‚tokens + å“åº”tokens + æç¤ºè¯æ¨¡æ¿ + ç¼“å†²
        # ä¸ºäº†å®‰å…¨ï¼Œè®¾ç½®è¯·æ±‚tokensä¸ºæ€»ä¸Šä¸‹æ–‡çš„30%ï¼Œå“åº”tokensä¸º60%ï¼Œç•™20%ç¼“å†²
        self.max_context_tokens = max_context_tokens
        
        # æ„å»ºåŠ¨æ€è¯„ä¼°æç¤ºè¯æ¨¡æ¿
        self.evaluation_prompt_template = self._build_dynamic_prompt_template()
        
        # è®¡ç®—æç¤ºè¯æ¨¡æ¿çš„åŸºç¡€tokenæ•°é‡ï¼ˆä¸åŒ…æ‹¬åŠ¨æ€å†…å®¹ï¼‰
        template_base = self.evaluation_prompt_template.replace('{test_case_id}', '').replace('{test_case_title}', '').replace('{prerequisites}', '').replace('{step_description}', '').replace('{expected_result}', '').replace('{test_cases_json}', '')
        self.template_base_tokens = self.token_counter.count_tokens(template_base)
        
        # é‡æ–°è®¡ç®—tokené…ç½®ï¼Œå°†æ¨¡æ¿tokensçº³å…¥è€ƒè™‘
        available_tokens = max_context_tokens - self.template_base_tokens  # å‡å»æ¨¡æ¿å ç”¨çš„tokens
        self.max_request_tokens = int(available_tokens * 0.3)  # 30%ç”¨äºè¯·æ±‚ï¼ˆç”¨ä¾‹æ•°æ®éƒ¨åˆ†ï¼‰
        self.max_response_tokens = int(available_tokens * 0.6)  # 60%ç”¨äºå“åº”
        self.token_threshold = int(self.max_request_tokens * 0.8)  # 80%é˜ˆå€¼ï¼Œç¡®ä¿å®‰å…¨
        
        print(f"Tokené…ç½®: æ€»ä¸Šä¸‹æ–‡={max_context_tokens}, æ¨¡æ¿åŸºç¡€tokens={self.template_base_tokens}, å¯ç”¨tokens={available_tokens}")
        print(f"è¯·æ±‚é™åˆ¶={self.max_request_tokens}, å“åº”é™åˆ¶={self.max_response_tokens}, è¯·æ±‚é˜ˆå€¼={self.token_threshold}")

    def _build_dynamic_prompt_template(self) -> str:
        """
        æ ¹æ®è‡ªå®šä¹‰è§„åˆ™é…ç½®æ„å»ºåŠ¨æ€æç¤ºè¯æ¨¡æ¿
        
        è¿”å›:
            str: åŠ¨æ€ç”Ÿæˆçš„æç¤ºè¯æ¨¡æ¿
        """
        title_max_length = self.rules_config['title_max_length']
        max_steps = self.rules_config['max_steps']
        priority_ratios = self.rules_config['priority_ratios']
        
        # æ„å»ºä¼˜å…ˆçº§å æ¯”è¯´æ˜
        p0_range = f"{priority_ratios['P0']['min']}%~{priority_ratios['P0']['max']}%"
        p1_range = f"{priority_ratios['P1']['min']}%~{priority_ratios['P1']['max']}%"
        p2_range = f"{priority_ratios['P2']['min']}%~{priority_ratios['P2']['max']}%"
        
        template = f"""è¯·æ ¹æ®ç»™å‡ºçš„è¯„åˆ†è§„åˆ™ï¼Œä¸ºç”¨ä¾‹æ•°æ®æ‰“åˆ†ï¼Œå¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®ã€‚**é™¤æ­¤ä»¥å¤–ä¸éœ€è¦ä»»ä½•åˆ†ææˆ–è§£é‡Š**ã€‚

è¯„åˆ†è§„åˆ™ï¼š

| ç”¨ä¾‹è¦ç´  | æ˜¯å¦å¿…é¡» | è¦æ±‚                                                                                                              |
| ---- | ---- | --------------------------------------------------------------------------------------------------------------- |
| å…³è”éœ€æ±‚ | æ˜¯    | 1. ç”¨ä¾‹åº”å½“ä¸çˆ¶éœ€æ±‚å•å…³è”                                                                                                     |
| ç”¨ä¾‹æ ‡é¢˜ | æ˜¯    | 1. æ ‡é¢˜é•¿åº¦ä¸è¶…è¿‡ {title_max_length} å­—ç¬¦ï¼Œä¸”æè¿°æµ‹è¯•åŠŸèƒ½ç‚¹<br>2. è¯­è¨€æ¸…æ™°ï¼Œç®€æ´æ˜“æ‡‚<br>3. é¿å…åœ¨ç”¨ä¾‹è¯„åˆ†ä¸­å‡ºç°æ­¥éª¤                                                    |
| å‰ç½®æ¡ä»¶ | å¦    | 1. åˆ—å‡ºæ‰€æœ‰å‰æï¼šè´¦å·ç±»å‹ï¼Œç°åº¦ç­‰<br>2. é¿å…è¿‡åº¦å¤æ‚ï¼šæ¯ä¸ªæ¡ä»¶ä¸è¶…è¿‡ 2 é¡¹æè¿°ï¼Œå¿…è¦æ—¶åˆ†ç‚¹åˆ—å‡º                                                           |
| æµ‹è¯•æ­¥éª¤ | æ˜¯    | 1. æ­¥éª¤ç”¨ç¼–å·é“¾æ¥ï¼šä½¿ç”¨ 1ã€2ã€3... ç»“æ„ï¼Œæ¯æ­¥æè¿°ä¸€ä¸ªåŠ¨ä½œ<br>2. æ­¥éª¤å…·ä½“åŒ–ï¼šåŒ…å«ç”¨æˆ·æ“ä½œã€è¾“å…¥å€¼å’Œä¸Šä¸‹æ–‡è¯´æ˜<br>3. æ­¥éª¤æ•°åˆç†ï¼šä¸è¶…è¿‡{max_steps}æ­¥ï¼Œå¦åˆ™éœ€åˆ†è§£ä¸ºå¤šä¸ªç”¨ä¾‹ã€‚<br>4. é¿å…æ­¥éª¤ä¸­å¸¦æœ‰æ£€æŸ¥ç‚¹ |
| é¢„æœŸç»“æœ | æ˜¯    | 1. æè¿°æ˜ç¡®ç»“æœï¼šæ˜ç¡®çš„ç»“æœï¼Œç¡®åˆ‡çš„æ£€æŸ¥<br>2. é¿å…æ¨¡æ£±ä¸¤å¯çš„è¯ï¼ˆå¦‚åŠŸèƒ½æ­£å¸¸ï¼Œè·Ÿç°ç½‘ä¸€è‡´ç­‰ï¼‰                                                              |
| ä¼˜å…ˆçº§  | æ˜¯    | 1. ç­‰çº§åˆ’åˆ†æ ‡å‡†ï¼šé‡‡ç”¨ P0-P2<br>2. æ˜ç¡®æ ‡æ³¨ï¼šæ¯ä¸ªç”¨ä¾‹å¿…é¡»æœ‰ä¼˜å…ˆçº§å­—æ®µã€‚<br>3. ä¼˜å…ˆçº§æ¯”ä¾‹ï¼šP0å æ¯”åº”åœ¨{p0_range}ä¹‹é—´ï¼ŒP1å æ¯”{p1_range}ï¼ŒP2å æ¯”{p2_range}                  |

è¯„åˆ†æ»¡åˆ†ä¸º 10 åˆ†ï¼Œæœªæä¾›å¿…é¡»æä¾›çš„å­—æ®µæ‰£ä¸€åˆ†ï¼Œæ¯æœ‰ä¸€ç‚¹è¦æ±‚æœªæ»¡è¶³æ‰£ä¸€åˆ†ï¼Œæœ€ä½ 0 åˆ†ã€‚
å¯¹ä½äº 10 åˆ†çš„è¦ç´ ç»™å‡ºç®€è¦çš„å»ºè®®ã€‚ç”±äºæ•°æ®é‡è¾ƒå¤§ï¼Œç»™å‡ºçš„æ¯ä¸€æ¡å»ºè®®éƒ½åº”å½“ç®€æ´å‡ç»ƒã€‚

å¯¹äºæ¯ä¸€æ¡ç”¨ä¾‹ï¼Œåœ¨ä½ çš„å›ç­”ä¸­è¯„åˆ†ä¸å»ºè®®çš„æ ¼å¼å¦‚ä¸‹ï¼ˆä½ çš„å›ç­”åªéœ€æä¾›æ­¤ç±»è¡¨æ ¼ï¼Œåº”ä¸¥æ ¼æŒ‰ç…§æ­¤æ ‡å‡†æ‰§è¡Œï¼‰ï¼š

| ç”¨ä¾‹ä¿¡æ¯                           | åˆ†æ•°  | æ”¹è¿›å»ºè®®                |
| ------------------------------ | --- | ------------------- |
| **ç”¨ä¾‹ID**<br>{{test_case_id}}     | -   | -                   |
| **ç”¨ä¾‹æ ‡é¢˜**<br>{{test_case_title}}  | 8   | æ”¹ä¸º"éªŒè¯é”™è¯¯å¯†ç ç™»å½•çš„å¤±è´¥æç¤º"   |
| **å‰ç½®æ¡ä»¶**<br>{{prerequisites}}    | 8   | è¡¥å……ç³»ç»Ÿç‰ˆæœ¬è¦æ±‚            |
| **æµ‹è¯•æ­¥éª¤**<br>{{step_description}} | 6   | æ­¥éª¤3å¢åŠ "ç­‰å¾…3ç§’"         |
| **é¢„æœŸç»“æœ**<br>{{expected_result}}  | 7   | æ˜ç¡®æç¤ºä½ç½®ï¼ˆå¦‚ï¼šè¾“å…¥æ¡†ä¸‹æ–¹çº¢è‰²æ–‡å­—ï¼‰ |

æµ‹è¯•ç”¨ä¾‹ï¼š
{{test_cases_json}}
"""
        return template

    def estimate_batch_tokens(self, test_cases: List[Dict[str, Any]]) -> int:
        """
        ä¼°ç®—ä¸€æ‰¹æµ‹è¯•ç”¨ä¾‹éœ€è¦çš„tokenæ•°é‡
        
        å‚æ•°:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            
        è¿”å›:
            é¢„è®¡tokenæ•°é‡
        """
        total_tokens = 0
        
        # ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹ä¼°ç®—token
        for test_case in test_cases:
            # æ„å»ºå•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„æç¤ºè¯
            test_cases_json = json.dumps([test_case], ensure_ascii=False, indent=2)
            
            # æå–æµ‹è¯•ç”¨ä¾‹å­—æ®µ
            case_id = test_case.get('test_case_id', 'N/A')
            title = test_case.get('test_case_title', 'æœªæä¾›')
            prerequisites = test_case.get('prerequisites', 'æœªæä¾›') 
            steps = test_case.get('step_description', 'æœªæä¾›')
            expected = test_case.get('expected_result', 'æœªæä¾›')
            
            prompt = self.evaluation_prompt_template.format(
                test_case_id=case_id,
                test_case_title=title,
                prerequisites=prerequisites,
                step_description=steps,
                expected_result=expected,
                test_cases_json=test_cases_json
            )
            
            # è®¡ç®—tokenæ•°é‡
            tokens = self.token_counter.count_tokens(prompt)
            total_tokens += tokens
        
        return total_tokens
    
    def split_test_cases_by_tokens(self, test_cases: List[Dict[str, Any]], 
                                 start_index: int = 0) -> Tuple[List[Dict[str, Any]], int]:
        """
        æ ¹æ®tokené™åˆ¶åˆ†å‰²æµ‹è¯•ç”¨ä¾‹
        
        å‚æ•°:
            test_cases: å…¨éƒ¨æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            start_index: å¼€å§‹ç´¢å¼•
            
        è¿”å›:
            (å½“å‰æ‰¹æ¬¡çš„æµ‹è¯•ç”¨ä¾‹, ä¸‹ä¸€æ‰¹æ¬¡çš„å¼€å§‹ç´¢å¼•)
        """
        current_batch = []
        current_tokens = 0
        
        for i in range(start_index, len(test_cases)):
            # å°è¯•æ·»åŠ ä¸‹ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹
            candidate_batch = current_batch + [test_cases[i]]
            candidate_tokens = self.estimate_batch_tokens(candidate_batch)
            
            if candidate_tokens > self.token_threshold and current_batch:
                # è¶…è¿‡é˜ˆå€¼ä¸”å½“å‰æ‰¹æ¬¡ä¸ä¸ºç©ºï¼Œåœæ­¢æ·»åŠ 
                break
            
            # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
            current_batch.append(test_cases[i])
            current_tokens = candidate_tokens
            
        next_index = start_index + len(current_batch)
        
        # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡åŒ…å«çš„æµ‹è¯•ç”¨ä¾‹ID
        batch_ids = [case.get('test_case_id', 'N/A') for case in current_batch]
        print(f"å½“å‰æ‰¹æ¬¡åŒ…å« {len(current_batch)} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œé¢„è®¡ä½¿ç”¨ {current_tokens} tokens")
        print(f"æ‰¹æ¬¡åŒ…å«çš„ç”¨ä¾‹ID: {', '.join(batch_ids)}")
        
        return current_batch, next_index
    
    async def evaluate_batch(self, test_cases: List[Dict[str, Any]], 
                           session: aiohttp.ClientSession) -> str:
        """
        è¯„ä¼°ä¸€æ‰¹æµ‹è¯•ç”¨ä¾‹
        
        å‚æ•°:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            session: HTTPä¼šè¯
            
        è¿”å›:
            AIè¯„ä¼°ç»“æœ
        """
        # æ„å»ºæ‰¹é‡æç¤ºè¯ - ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
        test_cases_json = json.dumps(test_cases, ensure_ascii=False, indent=2)
        
        # æ„å»ºæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„ç»¼åˆä¿¡æ¯å­—ç¬¦ä¸²
        cases_info = ""
        for i, test_case in enumerate(test_cases, 1):
            case_id = test_case.get('test_case_id', 'N/A')
            title = test_case.get('test_case_title', 'æœªæä¾›')
            prerequisites = test_case.get('prerequisites', 'æœªæä¾›') 
            steps = test_case.get('step_description', 'æœªæä¾›')
            expected = test_case.get('expected_result', 'æœªæä¾›')
            
            cases_info += f"""
ç”¨ä¾‹ {i}:
- ç”¨ä¾‹ID: {case_id}
- ç”¨ä¾‹æ ‡é¢˜: {title}
- å‰ç½®æ¡ä»¶: {prerequisites}
- æµ‹è¯•æ­¥éª¤: {steps}
- é¢„æœŸç»“æœ: {expected}
"""
        
        # æ„å»ºæ‰¹é‡æç¤ºè¯
        prompt = self.evaluation_prompt_template.format(
            test_case_id="[æ‰¹é‡å¤„ç†]",
            test_case_title="[æ‰¹é‡å¤„ç†]",
            prerequisites="[æ‰¹é‡å¤„ç†]",
            step_description="[æ‰¹é‡å¤„ç†]",
            expected_result="[æ‰¹é‡å¤„ç†]",
            test_cases_json=test_cases_json
        )
        
        # æ·»åŠ ç‰¹æ®Šè¯´æ˜ï¼Œè¦æ±‚AIä¸ºæ¯ä¸ªç”¨ä¾‹ç”Ÿæˆå•ç‹¬çš„è¡¨æ ¼
        batch_instruction = f"""
è¯·ä¸ºä»¥ä¸‹ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹åˆ†åˆ«ç”Ÿæˆè¯„ä¼°è¡¨æ ¼ã€‚æ¯ä¸ªç”¨ä¾‹åº”è¯¥æœ‰è‡ªå·±ç‹¬ç«‹çš„è¡¨æ ¼ã€‚

{cases_info}

é‡è¦æç¤ºï¼š
1. è¯·ä¸ºæ¯ä¸ªç”¨ä¾‹ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„è¡¨æ ¼
2. æ¯ä¸ªè¡¨æ ¼éƒ½åº”è¯¥åŒ…å«è¡¨å¤´ "| ç”¨ä¾‹ä¿¡æ¯ | åˆ†æ•° | æ”¹è¿›å»ºè®® |"
3. æ¯ä¸ªè¡¨æ ¼ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
4. ä¸¥æ ¼æŒ‰ç…§æä¾›çš„è¡¨æ ¼æ ¼å¼è¾“å‡º
"""
        
        final_prompt = batch_instruction + "\n\n" + prompt
        
        # è®¡ç®—è¯·æ±‚tokenæ•°é‡ï¼ŒåŠ¨æ€è®¾ç½®å“åº”tokené™åˆ¶
        request_tokens = self.token_counter.count_tokens(final_prompt)
        # å“åº”tokenåº”è¯¥ä¸è¯·æ±‚tokenå¤§è‡´ç›¸ç­‰ï¼Œä½†ä¸è¶…è¿‡æœ€å¤§å“åº”é™åˆ¶
        dynamic_response_tokens = min(request_tokens * 2, self.max_response_tokens)  # æ‰¹é‡å¤„ç†éœ€è¦æ›´å¤šå“åº”ç©ºé—´
        
        print(f"æ‰¹é‡å¤„ç† {len(test_cases)} ä¸ªç”¨ä¾‹: è¯·æ±‚tokens={request_tokens}, å“åº”tokensé™åˆ¶={dynamic_response_tokens}")
        
        # æ˜¾ç¤ºæ­£åœ¨å¤„ç†çš„æµ‹è¯•ç”¨ä¾‹ID
        processing_ids = [case.get('test_case_id', 'N/A') for case in test_cases]
        print(f"ğŸ”„ æ­£åœ¨å¤„ç†çš„æµ‹è¯•ç”¨ä¾‹: {', '.join(processing_ids)}")
        print("ğŸ“¤ æ­£åœ¨è°ƒç”¨AIè¿›è¡Œè¯„ä¼°...")
        
        # è°ƒç”¨AI API
        result = await self.api_manager.call_llm(
            prompt=final_prompt,
            session=session,
            max_tokens=dynamic_response_tokens
        )
        
        print("âœ… AIè¯„ä¼°å®Œæˆï¼Œå¼€å§‹è§£æç»“æœ...")
        return result
    
    def parse_evaluation_result(self, ai_response: str) -> List[Dict[str, Any]]:
        """
        è§£æAIè¯„ä¼°ç»“æœï¼Œå°†Markdownè¡¨æ ¼è½¬æ¢ä¸ºJSON
        æ”¯æŒè§£æå¤šä¸ªè¡¨æ ¼ï¼ˆå½“AIè¿”å›å¤šä¸ªè¡¨æ ¼æ—¶ï¼‰
        
        å‚æ•°:
            ai_response: AIè¿”å›çš„Markdownè¡¨æ ¼
            
        è¿”å›:
            è§£æåçš„è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        evaluations = []
        
        # æ¸…ç†å“åº”ï¼Œæ‰¾åˆ°æ‰€æœ‰è¡¨æ ¼éƒ¨åˆ†
        lines = ai_response.split('\n')
        all_tables = []
        current_table_lines = []
        table_started = False
        
        for line in lines:
            line = line.strip()
            
            # æ£€æµ‹è¡¨æ ¼å¼€å§‹
            if '| ç”¨ä¾‹ä¿¡æ¯ |' in line or '| --- |' in line:
                # å¦‚æœå·²ç»åœ¨å¤„ç†è¡¨æ ¼ï¼Œå…ˆä¿å­˜å½“å‰è¡¨æ ¼
                if table_started and current_table_lines:
                    all_tables.append(current_table_lines[:])
                    current_table_lines = []
                table_started = True
                continue
            
            # è¡¨æ ¼å†…å®¹è¡Œ
            elif table_started and line.startswith('|') and len(line.split('|')) >= 4:
                current_table_lines.append(line)
            
            # è¡¨æ ¼ç»“æŸï¼ˆç©ºè¡Œæˆ–éè¡¨æ ¼è¡Œï¼‰
            elif table_started and (not line or not line.startswith('|')):
                if current_table_lines:
                    all_tables.append(current_table_lines[:])
                    current_table_lines = []
                table_started = False
        
        # å¤„ç†æœ€åä¸€ä¸ªè¡¨æ ¼
        if table_started and current_table_lines:
            all_tables.append(current_table_lines)
        
        print(f"æ‰¾åˆ° {len(all_tables)} ä¸ªè¡¨æ ¼")
        print("ğŸ” å¼€å§‹è§£æè¡¨æ ¼æ•°æ®...")
        
        if not all_tables:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¡¨æ ¼æ•°æ®")
            return evaluations
        
        # è§£ææ¯ä¸ªè¡¨æ ¼
        for table_index, table_lines in enumerate(all_tables):
            print(f"ğŸ“‹ è§£æç¬¬ {table_index + 1} ä¸ªè¡¨æ ¼ï¼ŒåŒ…å« {len(table_lines)} è¡Œ")
            
            current_case = None
            case_id = None
            
            for line in table_lines:
                parts = [part.strip() for part in line.split('|')[1:-1]]
                if len(parts) >= 3:
                    field_info = parts[0]
                    score = parts[1]
                    suggestion = parts[2]
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç”¨ä¾‹ID
                    if '**ç”¨ä¾‹ID**' in field_info:
                        # æå–ç”¨ä¾‹ID
                        if '<br>' in field_info:
                            id_part = field_info.split('<br>')[-1].strip()
                        else:
                            # å¤„ç†æ²¡æœ‰<br>çš„æƒ…å†µï¼Œç›´æ¥ä»field_infoä¸­æå–
                            id_part = field_info.replace('**ç”¨ä¾‹ID**', '').strip()
                        
                        case_id = id_part
                        print(f"  ğŸ“ æ­£åœ¨è§£æç”¨ä¾‹ID: {case_id}")
                        
                        # ä¿å­˜ä¹‹å‰çš„ç”¨ä¾‹
                        if current_case:
                            evaluations.append(current_case)
                        
                        # å¼€å§‹æ–°ç”¨ä¾‹
                        current_case = {
                            'test_case_id': case_id,
                            'evaluations': []
                        }
                        
                    elif current_case and '**' in field_info:
                        # æå–å­—æ®µåå’Œå†…å®¹ï¼Œæ­£ç¡®å¤„ç†å¤šè¡Œå†…å®¹ï¼ˆ<br>åˆ†éš”ï¼‰
                        if '<br>' in field_info:
                            field_parts = field_info.split('<br>')
                            field_name = field_parts[0].replace('**', '').strip()
                            # åˆå¹¶æ‰€æœ‰å†…å®¹éƒ¨åˆ†ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
                            field_content = '<br>'.join(field_parts[1:]).strip() if len(field_parts) > 1 else ''
                            # å°†<br>è½¬æ¢ä¸ºå®é™…æ¢è¡Œç¬¦ï¼Œä¾¿äºé˜…è¯»
                            field_content = field_content.replace('<br>', '\n')
                        else:
                            # å¤„ç†æ²¡æœ‰<br>çš„æƒ…å†µ
                            field_name = field_info.replace('**', '').strip()
                            field_content = ''
                        
                        print(f"    ğŸ“Š è§£æå­—æ®µ: {field_name} (åˆ†æ•°: {score.strip() if score.strip() != '-' else 'æ— '})")
                        
                        evaluation_item = {
                            'field': field_name,
                            'content': field_content,
                            'score': score.strip() if score.strip() != '-' else None,
                            'suggestion': suggestion.strip() if suggestion.strip() != '-' else None
                        }
                        
                        current_case['evaluations'].append(evaluation_item)
            
            # æ·»åŠ å½“å‰è¡¨æ ¼çš„æœ€åä¸€ä¸ªç”¨ä¾‹
            if current_case:
                evaluations.append(current_case)
                print(f"  âœ… å®Œæˆç”¨ä¾‹è§£æ: {current_case['test_case_id']}")
        
        print(f"ğŸ‰ æˆåŠŸè§£æ {len(evaluations)} ä¸ªç”¨ä¾‹çš„è¯„ä¼°ç»“æœ")
        if evaluations:
            # è¾“å‡ºè§£æç»“æœçš„ç¤ºä¾‹
            first_case = evaluations[0]
            print(f"ğŸ“Š ç¤ºä¾‹è§£æç»“æœ - ç”¨ä¾‹ID: {first_case['test_case_id']}, è¯„ä¼°é¡¹æ•°: {len(first_case['evaluations'])}")
            for item in first_case['evaluations'][:2]:  # æ˜¾ç¤ºå‰ä¸¤ä¸ªè¯„ä¼°é¡¹
                print(f"    - {item['field']}: åˆ†æ•°={item['score']}, å»ºè®®={item['suggestion']}")
        
        return evaluations
    
    async def evaluate_test_cases(self, test_cases: List[Dict[str, Any]], 
                                test_batch_count: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è¯„ä¼°æµ‹è¯•ç”¨ä¾‹
        
        å‚æ•°:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            test_batch_count: æµ‹è¯•æ•°æ®æ‰¹æ¬¡ï¼Œ1è¡¨ç¤ºåªå¤„ç†ç¬¬ä¸€æ‰¹
            
        è¿”å›:
            è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        all_evaluations = []
        current_index = 0
        batch_number = 1
        
        async with aiohttp.ClientSession() as session:
            while current_index < len(test_cases):
                # å¦‚æœè®¾ç½®äº†æµ‹è¯•æ‰¹æ¬¡é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦è¶…è¿‡
                if test_batch_count and batch_number > test_batch_count:
                    print(f"ğŸ›‘ è¾¾åˆ°æµ‹è¯•æ‰¹æ¬¡é™åˆ¶ ({test_batch_count})ï¼Œåœæ­¢å¤„ç†")
                    break
                
                # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
                remaining_cases = len(test_cases) - current_index
                progress_percent = (current_index / len(test_cases)) * 100
                print(f"\nğŸ“Š æ€»ä½“è¿›åº¦: {current_index}/{len(test_cases)} ({progress_percent:.1f}%), å‰©ä½™ {remaining_cases} ä¸ªç”¨ä¾‹")
                print(f"ğŸš€ å¼€å§‹å¤„ç†ç¬¬ {batch_number} æ‰¹æ¬¡...")
                
                # åˆ†å‰²å½“å‰æ‰¹æ¬¡
                batch_cases, next_index = self.split_test_cases_by_tokens(
                    test_cases, current_index
                )
                
                if not batch_cases:
                    print("âœ… æ²¡æœ‰æ›´å¤šæµ‹è¯•ç”¨ä¾‹å¯å¤„ç†")
                    break
                
                try:
                    # è¯„ä¼°å½“å‰æ‰¹æ¬¡
                    ai_result = await self.evaluate_batch(batch_cases, session)
                    print(f"ğŸ“„ AIè¿”å›ç»“æœé•¿åº¦: {len(ai_result)}")
                    print(f"ğŸ” AIè¿”å›ç»“æœå‰1000å­—ç¬¦é¢„è§ˆ: ========================================")
                    print(f"\n{ai_result[:1000]}\n......")
                    print("============================================================")
                    
                    # è§£æç»“æœ
                    batch_evaluations = self.parse_evaluation_result(ai_result)
                    all_evaluations.extend(batch_evaluations)
                    
                    # æ˜¾ç¤ºæœ¬æ‰¹æ¬¡å¤„ç†çš„ç”¨ä¾‹ID
                    processed_ids = [eval_result['test_case_id'] for eval_result in batch_evaluations]
                    print(f"âœ… ç¬¬ {batch_number} æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œè¯„ä¼°äº† {len(batch_evaluations)} ä¸ªç”¨ä¾‹")
                    print(f"ğŸ“‹ å·²å®Œæˆè¯„ä¼°çš„ç”¨ä¾‹ID: {', '.join(processed_ids)}")
                    
                    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ä¸”ç¬¬ä¸€æ‰¹æ¬¡å®Œæˆï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
                    if test_batch_count == 1 and batch_number == 1:
                        print(f"\nğŸ§ª ç¬¬ä¸€æ‰¹æ¬¡æµ‹è¯•å®Œæˆï¼Œè¯„ä¼°ç»“æœé¢„è§ˆ:")
                        if batch_evaluations:
                            first_eval = batch_evaluations[0]
                            print(f"ğŸ“Š ç”¨ä¾‹ID: {first_eval['test_case_id']}")
                            print(f"ğŸ“ˆ è¯„ä¼°é¡¹æ•°é‡: {len(first_eval['evaluations'])}")
                        else:
                            print("âŒ è§£æè¯„ä¼°ç»“æœå¤±è´¥ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è§£æé€»è¾‘")
                            print(f"ğŸ” AIåŸå§‹è¿”å›: {ai_result[:1000]}...")
                        print("\nğŸ’¡ å¦‚éœ€å¤„ç†æ›´å¤šæ‰¹æ¬¡ï¼Œè¯·ä¿®æ”¹ test_batch_count å‚æ•°")
                        break
                    
                except Exception as e:
                    print(f"âŒ ç¬¬ {batch_number} æ‰¹æ¬¡å¤„ç†å¤±è´¥: {str(e)}")
                    if test_batch_count == 1:
                        print("ğŸ”§ æµ‹è¯•æ‰¹æ¬¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®å’Œç½‘ç»œè¿æ¥")
                        break
                    else:
                        print("â­ï¸ è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡")
                
                current_index = next_index
                batch_number += 1
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                await asyncio.sleep(1)
        
        return all_evaluations


async def main_process(test_batch_count: int = 3):
    """
    ä¸»å¤„ç†æµç¨‹
    
    å‚æ•°:
        test_batch_count: æµ‹è¯•æ•°æ®æ‰¹æ¬¡ï¼Œ3è¡¨ç¤ºåªå¤„ç†ç¬¬ä¸€æ‰¹ç”¨äºæµ‹è¯•
    """
    config = get_config()
    processor = TestCaseProcessor()
    # 12Kæ€»ä¸Šä¸‹æ–‡ï¼š3.6Kè¯·æ±‚ + 7.2Kå“åº” + 2.4Kç¼“å†²
    evaluator = TestCaseEvaluator(max_context_tokens=12000)
    
    # æ–‡ä»¶è·¯å¾„
    excel_file = config.local_data_path / "TestCase_20250717141033-32202633.xlsx"
    json_file = config.local_data_path / "TestCase_20250717141033-32202633.json"
    result_file = config.local_data_path / "Proceed_TestCase_20250717141033-32202633.json"
    
    try:
        # æ­¥éª¤1: æ£€æŸ¥Excelæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not excel_file.exists():
            raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_file}")
        
        # æ­¥éª¤2: å°†Excelè½¬æ¢ä¸ºJSONï¼ˆå¦‚æœJSONæ–‡ä»¶ä¸å­˜åœ¨ï¼‰
        if not json_file.exists():
            print("å¼€å§‹è½¬æ¢Excelæ–‡ä»¶ä¸ºJSONæ ¼å¼...")
            test_cases = processor.excel_to_json(str(excel_file), str(json_file))
        else:
            print("JSONæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½...")
            with open(json_file, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
        
        print(f"åŠ è½½äº† {len(test_cases)} æ¡æµ‹è¯•ç”¨ä¾‹æ•°æ®")
        
        # æ­¥éª¤3: AIè¯„ä¼°
        print(f"\nğŸš€ å¼€å§‹AIè¯„ä¼°ï¼Œæµ‹è¯•æ‰¹æ¬¡é™åˆ¶: {test_batch_count}")
        print(f"ğŸ“Š æ€»è®¡éœ€è¦è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        evaluations = await evaluator.evaluate_test_cases(test_cases, test_batch_count)
        
        # æ­¥éª¤4: ä¿å­˜è¯„ä¼°ç»“æœ
        if evaluations:
            file_manager = get_file_manager()
            # åŒ…è£…ä¸ºå­—å…¸æ ¼å¼ä»¥ç¬¦åˆsave_json_dataçš„æ¥å£
            result_data = {
                "evaluation_results": evaluations,
                "total_count": len(evaluations),
                "generated_at": str(json_file).split('_')[-1].replace('.json', '')
            }
            file_manager.save_json_data(result_data, str(result_file))
            print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            print(f"ğŸ“ˆ å…±è¯„ä¼°äº† {len(evaluations)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        else:
            print("\nâŒ æ²¡æœ‰ç”Ÿæˆè¯„ä¼°ç»“æœ")
            
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {str(e)}")
        raise


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†3æ‰¹æ•°æ®
    asyncio.run(main_process(test_batch_count=3))
