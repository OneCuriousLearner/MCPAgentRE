"""
æµ‹è¯•ç”¨ä¾‹è´¨é‡è¯„åˆ†ç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯é…ç½®ç®¡ç†å’Œè¯„åˆ†åŠŸèƒ½çš„æ­£ç¡®æ€§
"""

import asyncio
import json
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mcp_tools.scoring_config_manager import ScoringConfigManager, ScoringConfig
from mcp_tools.testcase_quality_scorer import TestCaseQualityScorer


async def test_config_manager():
    """æµ‹è¯•é…ç½®ç®¡ç†å™¨åŠŸèƒ½"""
    print("=== æµ‹è¯•é…ç½®ç®¡ç†å™¨ ===")
    
    config_manager = ScoringConfigManager("test/test_config.json")
    
    # æµ‹è¯•åŠ è½½é»˜è®¤é…ç½®
    print("1. åŠ è½½é»˜è®¤é…ç½®...")
    config = await config_manager.load_config()
    print(f"   ç‰ˆæœ¬: {config.version}")
    print(f"   æ ‡é¢˜æƒé‡: {config.title_rule.weight}")
    print(f"   æ ‡é¢˜æœ€å¤§é•¿åº¦: {config.title_rule.max_length}")
    
    # æµ‹è¯•é…ç½®æ‘˜è¦
    print("\n2. è·å–é…ç½®æ‘˜è¦...")
    summary = await config_manager.get_config_summary()
    print(f"   æ‘˜è¦: {json.dumps(summary, ensure_ascii=False, indent=2)}")
    
    # æµ‹è¯•æ›´æ–°è§„åˆ™
    print("\n3. æ›´æ–°æ ‡é¢˜è§„åˆ™...")
    new_title_rule = {
        "max_length": 50,
        "min_length": 8,
        "weight": 0.25
    }
    await config_manager.update_rule("title", new_title_rule)
    updated_config = await config_manager.load_config()
    print(f"   æ›´æ–°åçš„æ ‡é¢˜æœ€å¤§é•¿åº¦: {updated_config.title_rule.max_length}")
    print(f"   æ›´æ–°åçš„æ ‡é¢˜æƒé‡: {updated_config.title_rule.weight}")
    
    # æµ‹è¯•é…ç½®éªŒè¯
    print("\n4. æµ‹è¯•é…ç½®éªŒè¯...")
    test_config = {
        "version": "1.0",
        "title_rule": {"max_length": 60, "weight": 0.3},
        "precondition_rule": {"max_count": 3, "weight": 0.1},
        "steps_rule": {"min_steps": 1, "weight": 0.3},
        "expected_result_rule": {"weight": 0.2},
        "priority_rule": {"weight": 0.1}
    }
    
    validation_result = await config_manager.validate_config(test_config)
    print(f"   éªŒè¯ç»“æœ: {json.dumps(validation_result, ensure_ascii=False, indent=2)}")
    
    # æµ‹è¯•é‡ç½®é…ç½®
    print("\n5. é‡ç½®ä¸ºé»˜è®¤é…ç½®...")
    await config_manager.reset_to_default()
    reset_config = await config_manager.load_config()
    print(f"   é‡ç½®åçš„æ ‡é¢˜æœ€å¤§é•¿åº¦: {reset_config.title_rule.max_length}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if os.path.exists("test/test_config.json"):
        os.remove("test/test_config.json")
    
    print("âœ“ é…ç½®ç®¡ç†å™¨æµ‹è¯•å®Œæˆ")


async def test_quality_scorer():
    """æµ‹è¯•è´¨é‡è¯„åˆ†å™¨åŠŸèƒ½"""
    print("\n=== æµ‹è¯•è´¨é‡è¯„åˆ†å™¨ ===")
    
    scorer = TestCaseQualityScorer()
    await scorer.initialize()
    
    # æµ‹è¯•ç”¨ä¾‹æ•°æ®
    test_cases = [
        {
            "id": "TC001",
            "title": "ç™»å½•åŠŸèƒ½æµ‹è¯•",
            "precondition": "ç”¨æˆ·å·²æ³¨å†Œ",
            "steps": "1. æ‰“å¼€ç™»å½•é¡µé¢\n2. è¾“å…¥ç”¨æˆ·åå’Œå¯†ç \n3. ç‚¹å‡»ç™»å½•æŒ‰é’®",
            "expected_result": "ç™»å½•æˆåŠŸï¼Œè·³è½¬åˆ°é¦–é¡µ",
            "priority": "P1"
        },
        {
            "id": "TC002",
            "title": "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„æµ‹è¯•ç”¨ä¾‹æ ‡é¢˜ï¼Œå¯èƒ½ä¼šè¶…è¿‡æ¨èçš„é•¿åº¦é™åˆ¶ï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦ä¼šè¢«æ­£ç¡®è¯„åˆ†",
            "precondition": "",
            "steps": "ç‚¹å‡»æŒ‰é’®",
            "expected_result": "æ­£å¸¸",
            "priority": "High"
        },
        {
            "id": "TC003",
            "title": "ç”¨æˆ·ä¿¡æ¯æŸ¥è¯¢éªŒè¯æµ‹è¯•",
            "precondition": "1. ç”¨æˆ·å·²ç™»å½•ç³»ç»Ÿ\n2. ç”¨æˆ·æœ‰æŸ¥è¯¢æƒé™",
            "steps": "1. è¿›å…¥ç”¨æˆ·ç®¡ç†é¡µé¢\n2. é€‰æ‹©æŸ¥è¯¢æ¡ä»¶\n3. ç‚¹å‡»æŸ¥è¯¢æŒ‰é’®\n4. éªŒè¯æŸ¥è¯¢ç»“æœ",
            "expected_result": "ç³»ç»Ÿè¿”å›ç¬¦åˆæ¡ä»¶çš„ç”¨æˆ·ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«ç”¨æˆ·IDã€å§“åã€çŠ¶æ€ç­‰å­—æ®µï¼Œæ•°æ®å‡†ç¡®æ— è¯¯",
            "priority": "P2"
        }
    ]
    
    # æµ‹è¯•å•ä¸ªç”¨ä¾‹è¯„åˆ†
    print("1. æµ‹è¯•å•ä¸ªç”¨ä¾‹è¯„åˆ†...")
    for i, testcase in enumerate(test_cases):
        print(f"\n   æµ‹è¯•ç”¨ä¾‹ {i+1}: {testcase['title'][:20]}...")
        result = await scorer.score_single_testcase(testcase)
        print(f"   æ€»åˆ†: {result['total_score']}")
        print(f"   ç­‰çº§: {result['score_level']}")
        print(f"   æ ‡é¢˜åˆ†æ•°: {result['detailed_scores']['title']['score']}")
        print(f"   æ­¥éª¤åˆ†æ•°: {result['detailed_scores']['steps']['score']}")
        print(f"   æ”¹è¿›å»ºè®®æ•°: {len(result['improvement_suggestions'])}")
        
        if result['improvement_suggestions']:
            print(f"   ä¸»è¦å»ºè®®: {result['improvement_suggestions'][0]}")
    
    # æµ‹è¯•æ‰¹é‡è¯„åˆ†
    print("\n2. æµ‹è¯•æ‰¹é‡è¯„åˆ†...")
    batch_result = await scorer.score_batch_testcases(test_cases, batch_size=2)
    print(f"   æ€»æ•°: {batch_result['total_count']}")
    print(f"   æˆåŠŸæ•°: {batch_result['success_count']}")
    print(f"   å¹³å‡åˆ†: {batch_result['average_score']}")
    print(f"   åˆ†æ•°åˆ†å¸ƒ: {batch_result['score_distribution']}")
    
    print("âœ“ è´¨é‡è¯„åˆ†å™¨æµ‹è¯•å®Œæˆ")


async def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µå’Œå¼‚å¸¸å¤„ç†"""
    print("\n=== æµ‹è¯•è¾¹ç•Œæƒ…å†µ ===")
    
    scorer = TestCaseQualityScorer()
    await scorer.initialize()
    
    # æµ‹è¯•ç©ºæ•°æ®
    print("1. æµ‹è¯•ç©ºæ•°æ®...")
    empty_case = {
        "id": "TC_EMPTY",
        "title": "",
        "precondition": "",
        "steps": "",
        "expected_result": "",
        "priority": ""
    }
    
    result = await scorer.score_single_testcase(empty_case)
    print(f"   ç©ºæ•°æ®æ€»åˆ†: {result['total_score']}")
    print(f"   æ”¹è¿›å»ºè®®æ•°: {len(result['improvement_suggestions'])}")
    
    # æµ‹è¯•æé•¿æ•°æ®
    print("\n2. æµ‹è¯•æé•¿æ•°æ®...")
    long_case = {
        "id": "TC_LONG",
        "title": "è¿™æ˜¯ä¸€ä¸ªæå…¶è¯¦ç»†å’Œå†—é•¿çš„æµ‹è¯•ç”¨ä¾‹æ ‡é¢˜ï¼ŒåŒ…å«äº†å¤§é‡çš„æè¿°æ€§æ–‡å­—ï¼Œç›®çš„æ˜¯æµ‹è¯•ç³»ç»Ÿå¯¹äºè¿‡é•¿æ ‡é¢˜çš„å¤„ç†èƒ½åŠ›å’Œè¯„åˆ†é€»è¾‘",
        "precondition": "å‰ç½®æ¡ä»¶1ï¼šç”¨æˆ·å·²ç»å®Œæˆäº†ç³»ç»Ÿæ³¨å†Œ\nå‰ç½®æ¡ä»¶2ï¼šç”¨æˆ·å·²ç»é€šè¿‡äº†èº«ä»½éªŒè¯\nå‰ç½®æ¡ä»¶3ï¼šç³»ç»Ÿå¤„äºæ­£å¸¸è¿è¡ŒçŠ¶æ€",
        "steps": "1. æ‰§è¡Œç¬¬ä¸€æ­¥æ“ä½œ\n2. æ‰§è¡Œç¬¬äºŒæ­¥æ“ä½œ\n3. æ‰§è¡Œç¬¬ä¸‰æ­¥æ“ä½œ\n4. æ‰§è¡Œç¬¬å››æ­¥æ“ä½œ\n5. æ‰§è¡Œç¬¬äº”æ­¥æ“ä½œ\n6. æ‰§è¡Œç¬¬å…­æ­¥æ“ä½œ\n7. æ‰§è¡Œç¬¬ä¸ƒæ­¥æ“ä½œ\n8. æ‰§è¡Œç¬¬å…«æ­¥æ“ä½œ\n9. æ‰§è¡Œç¬¬ä¹æ­¥æ“ä½œ\n10. æ‰§è¡Œç¬¬åæ­¥æ“ä½œ",
        "expected_result": "ç³»ç»Ÿåº”è¯¥èƒ½å¤Ÿæ­£ç¡®å¤„ç†æ‰€æœ‰çš„è¾“å…¥æ•°æ®ï¼Œå¹¶è¿”å›å‡†ç¡®çš„ç»“æœä¿¡æ¯ï¼ŒåŒæ—¶ä¿è¯æ•°æ®çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼Œç¡®ä¿ç”¨æˆ·ä½“éªŒè‰¯å¥½",
        "priority": "P0"
    }
    
    result = await scorer.score_single_testcase(long_case)
    print(f"   é•¿æ•°æ®æ€»åˆ†: {result['total_score']}")
    print(f"   æ ‡é¢˜è¯„åˆ†: {result['detailed_scores']['title']['score']}")
    print(f"   å‰ç½®æ¡ä»¶è¯„åˆ†: {result['detailed_scores']['precondition']['score']}")
    
    # æµ‹è¯•å¼‚å¸¸æ•°æ®
    print("\n3. æµ‹è¯•å¼‚å¸¸æ•°æ®...")
    try:
        invalid_case = {
            "id": "TC_INVALID",
            "title": None,
            "precondition": 123,
            "steps": [],
            "expected_result": {"invalid": "data"},
            "priority": "INVALID"
        }
        
        result = await scorer.score_single_testcase(invalid_case)
        print(f"   å¼‚å¸¸æ•°æ®å¤„ç†æˆåŠŸï¼Œæ€»åˆ†: {result['total_score']}")
    except Exception as e:
        print(f"   å¼‚å¸¸æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
    
    print("âœ“ è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ")


async def test_performance():
    """æµ‹è¯•æ€§èƒ½è¡¨ç°"""
    print("\n=== æµ‹è¯•æ€§èƒ½è¡¨ç° ===")
    
    scorer = TestCaseQualityScorer()
    await scorer.initialize()
    
    # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
    print("1. ç”Ÿæˆæµ‹è¯•æ•°æ®...")
    test_cases = []
    for i in range(50):
        test_cases.append({
            "id": f"TC_{i:03d}",
            "title": f"æµ‹è¯•ç”¨ä¾‹æ ‡é¢˜{i}",
            "precondition": f"å‰ç½®æ¡ä»¶{i}",
            "steps": f"1. æ­¥éª¤1\n2. æ­¥éª¤2\n3. æ­¥éª¤3",
            "expected_result": f"é¢„æœŸç»“æœ{i}",
            "priority": "P1"
        })
    
    # æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½
    print("2. æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½...")
    import time
    start_time = time.time()
    
    batch_result = await scorer.score_batch_testcases(test_cases, batch_size=10)
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"   å¤„ç†äº† {batch_result['total_count']} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"   æ€»è€—æ—¶: {processing_time:.2f} ç§’")
    print(f"   å¹³å‡è€—æ—¶: {processing_time/batch_result['total_count']:.3f} ç§’/ç”¨ä¾‹")
    print(f"   æˆåŠŸç‡: {batch_result['success_count']/batch_result['total_count']*100:.1f}%")
    
    print("âœ“ æ€§èƒ½æµ‹è¯•å®Œæˆ")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•ç”¨ä¾‹è´¨é‡è¯„åˆ†ç³»ç»Ÿ...")
    
    try:
        await test_config_manager()
        await test_quality_scorer()
        await test_edge_cases()
        await test_performance()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())