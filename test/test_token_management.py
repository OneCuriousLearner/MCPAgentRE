"""
Tokenç®¡ç†é€»è¾‘éªŒè¯è„šæœ¬

éªŒè¯ä¿®æ”¹åçš„åŠ¨æ€tokenåˆ†é…ç­–ç•¥
"""

import sys
import json
import asyncio
import aiohttp
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# æ·»åŠ mcp_toolsç›®å½•åˆ°sys.path
mcp_tools_path = project_root / "mcp_tools"
if str(mcp_tools_path) not in sys.path:
    sys.path.insert(0, str(mcp_tools_path))

from common_utils import get_config
from mcp_tools.test_case_evaluator import TokenCounter, TestCaseEvaluator


async def test_token_management():
    """æµ‹è¯•tokenç®¡ç†é€»è¾‘"""
    print("ğŸ§ª æµ‹è¯•åŠ¨æ€tokenç®¡ç†ç­–ç•¥...")
    
    # åˆå§‹åŒ–
    config = get_config()
    evaluator = TestCaseEvaluator(max_context_tokens=12000)
    
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  æ€»ä¸Šä¸‹æ–‡: {evaluator.max_context_tokens}")
    print(f"  è¯·æ±‚é™åˆ¶: {evaluator.max_request_tokens}")
    print(f"  å“åº”é™åˆ¶: {evaluator.max_response_tokens}")
    print(f"  è¯·æ±‚é˜ˆå€¼: {evaluator.token_threshold}")
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    json_file = config.local_data_path / "TestCase_20250717141033-32202633.json"
    
    if not json_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        return
    
    with open(json_file, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    
    print(f"\nğŸ“‹ åŠ è½½äº† {len(test_cases)} æ¡æµ‹è¯•ç”¨ä¾‹ï¼Œæµ‹è¯•å‰3æ¡")
    
    # æµ‹è¯•å‰3ä¸ªæµ‹è¯•ç”¨ä¾‹çš„tokenåˆ†é…
    for i in range(min(3, len(test_cases))):
        test_case = test_cases[i]
        
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i+1} (ID: {test_case.get('test_case_id', 'N/A')}) ---")
        
        # æ„å»ºæç¤ºè¯
        test_cases_json = json.dumps([test_case], ensure_ascii=False, indent=2)
        
        case_id = test_case.get('test_case_id', 'N/A')
        title = test_case.get('test_case_title', 'æœªæä¾›')
        prerequisites = test_case.get('prerequisites', 'æœªæä¾›') 
        steps = test_case.get('step_description', 'æœªæä¾›')
        expected = test_case.get('expected_result', 'æœªæä¾›')
        
        prompt = evaluator.evaluation_prompt_template.format(
            test_case_id=case_id,
            test_case_title=title,
            prerequisites=prerequisites,
            step_description=steps,
            expected_result=expected,
            test_cases_json=test_cases_json
        )
        
        # è®¡ç®—tokenæ•°é‡
        request_tokens = evaluator.token_counter.count_tokens(prompt)
        dynamic_response_tokens = min(request_tokens, evaluator.max_response_tokens)
        total_estimated_tokens = request_tokens + dynamic_response_tokens
        
        print(f"ğŸ“Š Tokenåˆ†æ:")
        print(f"  è¯·æ±‚tokens: {request_tokens}")
        print(f"  å“åº”tokensé™åˆ¶: {dynamic_response_tokens}")
        print(f"  æ€»è®¡é¢„ä¼°: {total_estimated_tokens}")
        print(f"  ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡: {total_estimated_tokens / evaluator.max_context_tokens * 100:.1f}%")
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if request_tokens > evaluator.max_request_tokens:
            print(f"âš ï¸  è¯·æ±‚tokensè¶…è¿‡é™åˆ¶ ({evaluator.max_request_tokens})")
        
        if total_estimated_tokens > evaluator.max_context_tokens:
            print(f"âš ï¸  æ€»tokensè¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶")
        else:
            print(f"âœ… Tokenåˆ†é…åˆç†")
        
        # æ˜¾ç¤ºæç¤ºè¯çš„å‰200å­—ç¬¦
        print(f"ğŸ“ æç¤ºè¯é¢„è§ˆ: {prompt[:200]}...")


def test_batch_splitting():
    """æµ‹è¯•æ‰¹æ¬¡åˆ†å‰²é€»è¾‘"""
    print("\nğŸ”„ æµ‹è¯•æ‰¹æ¬¡åˆ†å‰²é€»è¾‘...")
    
    config = get_config()
    evaluator = TestCaseEvaluator(max_context_tokens=12000)
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    json_file = config.local_data_path / "TestCase_20250717141033-32202633.json"
    
    if not json_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        return
    
    with open(json_file, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    
    print(f"ğŸ“‹ ä½¿ç”¨ {len(test_cases)} æ¡æµ‹è¯•ç”¨ä¾‹è¿›è¡Œæ‰¹æ¬¡åˆ†å‰²æµ‹è¯•")
    
    # æ¨¡æ‹Ÿæ‰¹æ¬¡åˆ†å‰²
    current_index = 0
    batch_number = 1
    total_cases_processed = 0
    
    while current_index < len(test_cases) and batch_number <= 5:  # é™åˆ¶æµ‹è¯•5ä¸ªæ‰¹æ¬¡
        print(f"\n--- æ‰¹æ¬¡ {batch_number} ---")
        
        # åˆ†å‰²å½“å‰æ‰¹æ¬¡
        batch_cases, next_index = evaluator.split_test_cases_by_tokens(
            test_cases, current_index
        )
        
        if not batch_cases:
            print("æ²¡æœ‰æ›´å¤šæµ‹è¯•ç”¨ä¾‹å¯å¤„ç†")
            break
        
        # ç»Ÿè®¡æ‰¹æ¬¡ä¿¡æ¯
        batch_tokens = evaluator.estimate_batch_tokens(batch_cases)
        cases_in_batch = len(batch_cases)
        avg_tokens_per_case = batch_tokens // cases_in_batch if cases_in_batch > 0 else 0
        
        print(f"  ğŸ“¦ æ‰¹æ¬¡å¤§å°: {cases_in_batch} ä¸ªç”¨ä¾‹")
        print(f"  ğŸ“Š æ€»tokens: {batch_tokens}")
        print(f"  ğŸ“ˆ å¹³å‡æ¯ç”¨ä¾‹: {avg_tokens_per_case} tokens")
        print(f"  ğŸ“‹ ç”¨ä¾‹èŒƒå›´: {current_index} - {next_index-1}")
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é˜ˆå€¼å†…
        if batch_tokens <= evaluator.token_threshold:
            print(f"  âœ… åœ¨é˜ˆå€¼å†… ({evaluator.token_threshold})")
        else:
            print(f"  âš ï¸  è¶…è¿‡é˜ˆå€¼ ({evaluator.token_threshold})")
        
        total_cases_processed += cases_in_batch
        current_index = next_index
        batch_number += 1
    
    print(f"\nğŸ“ˆ æ‰¹æ¬¡åˆ†å‰²æ€»ç»“:")
    print(f"  æ€»æ‰¹æ¬¡æ•°: {batch_number - 1}")
    print(f"  å¤„ç†ç”¨ä¾‹æ•°: {total_cases_processed}")
    print(f"  å‰©ä½™ç”¨ä¾‹æ•°: {len(test_cases) - total_cases_processed}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Tokenç®¡ç†é€»è¾‘éªŒè¯...")
    
    # æµ‹è¯•tokenç®¡ç†
    asyncio.run(test_token_management())
    
    # æµ‹è¯•æ‰¹æ¬¡åˆ†å‰²
    test_batch_splitting()
    
    print("\nâœ… Tokenç®¡ç†é€»è¾‘éªŒè¯å®Œæˆï¼")


if __name__ == "__main__":
    main()
